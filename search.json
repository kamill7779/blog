[{"title":"Socket Programming 实验启动教学：Liso 项目从启动到可解析请求","path":"/blog/2026/02/08/Socket Programming 实验启动教学：Liso 项目从启动到可解析请求/","content":"0. 前言本项目源于 CMU 课程作业项目 15-441: Computer Networks Project 1: A Web Server Called Liso。本文主要结合天津大学《计算机网络》课程中 Socket Programming 实验的具体要求进行讲解。 声明：本文为个人实验记录与理解总结，仅供参考。我更希望读者在理解原理的基础上自己实现，而不是直接复制粘贴本文代码（毕竟说要查重的）；不同环境版本下细节可能存在差异。若你发现文中错误，欢迎邮件联系我（请在邮件标题注明 LisoLab1），我会尽量核实并更新。 基于学校提供的参考资料、Socket 编程实践指导书，以及四周任务完成表，整体实验难度已经有所下降。但在这一前提下，项目的启动阶段依然是最容易卡住的部分。 因此，本文的目标是帮助我们在项目前期抓住重点，并对课程中尚未涉及或涉及较少的内容做补充说明，重点放在“如何顺利启动项目”这件事上。 实际上，项目的核心难点集中在第一周，以及第四周：在需要理解 IO 多路复用的前提下，还要完成较大规模的代码重构。相比之下，第二、第三周任务主要是根据 RFC 文档对协议文本做正确处理。 1. 项目启动流程1.1. 基础设施1：Docker先不着急上手代码。这个项目的基础设施是 Docker。 如果对 Docker 还不熟悉，我们可以先阅读这篇入门文章： “一劳永逸”的开发容器入门：Docker 容器 + VS Code 全流程，可以动手操作一下熟悉连接开发容器的流程。 如果希望使用 VS Code 连接开发容器开发，你还需要修改一下 Dockerfile ，完整修改如下： # base imageFROM debian:bookworm-slim# make necessary directoriesRUN apt-get update \\ apt-get -y install gcc flex bison build-essential siege apache2-utils libssl-dev \\ # change ApacheBench request HTTP version to 1.1 perl -pi -e s/HTTP\\/1.0/HTTP\\/1.1/g /usr/bin/abWORKDIR /home 使用原始 ubuntu 18.04 时，VS Code Remote Container 会因 glibc 版本问题无法连接；而部分较新 Ubuntu 标签在拉取或兼容性上容易出现额外问题，因此这里选择 debian:bookworm-slim 作为稳定方案。 随后创建并在 ./devcontainer 创建 devcontainer.json 配置（具体作用参考上面的文章）： name: liso, containerName: liso, build: dockerfile: ../DockerFile, context: .. , workspaceFolder: /home/project-1, workspaceMount: source=$localWorkspaceFolder,target=/home/project-1,type=bind,consistency=cached, runArgs: [-p, 127.0.0.1:8888:15441] 每次使用 VS Code 连接开发容器都能够按照要求进行挂载和端口映射了。 必须使用 runArgs: [-p, 127.0.0.1:8888:15441] ，是为了把容器内 15441 端口稳定、明确地映射到宿主机 IPv4 地址 127.0.0.1 (line 8888)，避免只走 VS Code 自动转发导致的 IPv6IPv4 访问不一致问题。 1.2. 项目架构1：服务端与客户端首先不去深究原因，我们来完成如下操作，进行项目的一个小启动帮助我们理解项目架构： 按下 Ctrl + ~ （ Esc 下面那个键）启动终端，这里我们需要两个终端： 确保我们在 /home/project-1 目录下: 任选其一，输入命令 make ，等待成功后输入 ./echo_server ，你会看到终端状态如下，且没有其他的输出： make 的输出只要不存在 error 这种词都是正常的，make 根据 Makefile 中的定义，自动执行了一些 bash 命令，为我们生成了可执行文件，和 Dockerfile 有着相似的作用。 此时如果比较细心，你会发现在 make 后生成了很多文件，他们有的是项目编译生成的源文件，有的是可执行文件，还有一部分是词法语法分析器生成的中间产物，在这里我们先不多介绍。 随后切换到一个新的终端，输入命令 ./echo_client localhost 9999 ，在你感觉好像卡住了之后随便输入点什么，效果是这样的： 这里的逻辑是：echo_client 会向 localhost:9999 发送我们刚刚写的消息，随后被 echo_server 监听到后重新发回给我们，因此你可以花几分钟实践初读一下代码，尝试理解一下这个流程。 因此你就可以理解我们这个项目的最终目的：浏览器充当 echo_client 的角色向 echo_server 发送课上所学的 HTTP 报文（在代码中，我们从 TCP 连接中得到报文的文本形式，然后在提取一些我们想要的信息） 1.3. 项目架构2：核心方法 parse()新建一个终端，随后输入： ./example samples/sample_request_example 你会看到如下输出： 注意：如果在这里出现 Parsing Failed 或随后 Segmentation fault，很可能不是业务逻辑写错，而是换行符格式不匹配。 **原因在于：**解析器按 HTTP 报文标准使用 CRLF（\\r ）作为行结束符；但很多编辑器或环境会把文本保存成仅有 LF（ ），导致词法规则无法识别请求行结束，从而解析失败。 这类问题在 Linux 和 Windows 之间尤其常见：LinuxUnix 默认使用 LF，Windows 传统上使用 CRLF。当我们在不同系统间拷贝样例文件、用不同编辑器保存，或者通过 Git 自动转换行尾时，文件内容看起来“没区别”，但底层字节序列已经改变。 如果出现这样的问题，我们可以这样操作： 然后再输入上述指令就能得到正确结果了。 如果不去阅读 example.c 的代码我们会认为可能是这样处理的：HTTP 请求不就是一段字符串吗？那我们直接找 “GET”，再找几个冒号换行空格，按位置截取不就好了。 这个思路在最简单样例里似乎可行，但一旦进入真实请求，它会很快失效。首先是格式问题。HTTP 报文并不是“固定模板字符串”，而是由方法、URI、版本、可选空白、多个首部行、可选请求体共同组成，字段顺序和数量都可能变化。只靠 strstr、strtok 一类“硬切字符串”的方式，面对额外空格、大小写差异、重复首部、管线请求（pipelining）或半包读取时，都会出现误判甚至越界。 其次是性能与可维护性问题。字符串全局搜索和反复拷贝在请求量上来后开销明显，而且逻辑分散在大量“if-else + 指针偏移”中，调试和扩展都很痛苦。 带着这个问题我们去阅读 example.c 的代码，我们会发现：主流程里几乎没有任何“手写字符串切割”逻辑，核心只有这一句： Request *request = parse(buf, readRet, fd_in); 也就是说，代码中open + read 读进来的原始报文，经过一次 parse() 调用，就被转换成了 Request 对象；后面的代码只是在打印 http_method、http_uri、http_version 和 headers。这和我们最开始“写一堆 strstrstrtok 手工切字符串”的想象完全不同：真正复杂的工作被封装进了解析器内部。 这也正是这个项目的关键点。我们接下来不是去堆更多字符串技巧，而是先要理解这样的几个问题： parse() 是如何把文本变成结构体的？ 为什么现在的结构不支持多行 Header ? 我们应该如何拓展？ 2. 代码解析2.1. 基础设施2：编译原理与 parse()2.1.1. 状态机与分帧点进 parse() 后，我们会先看到一开始有一段莫名其妙的代码，实际上这就是有限状态机的代码实现，但它其实是在解决一个非常现实的问题：我们必须先知道“头部到哪里结束了”，才能谈解析。HTTP1.1 里头部结束符是 \\r \\r ，所以这段状态机不是在“理解 GETPOST”，而是在做“报文分帧（framing）”。 为什么要这样处理，而不是直接丢给 yyparse()？ 避免把不完整数据交给语法解析器：网络读取天然可能出现半包。一次 recv 读到的可能只是 GET HTTP1.1\\r Ho。如果这时就调用语法解析就不能成功，而且这个失败是因为“数据还没收全”。先用状态机找 \\r \\r ，能保证进入语法层的是“完整头部”。 把“边界问题”和“语义问题”拆开 状态机只负责回答：头部是否完整、结束在哪。lexyacc 才负责回答：方法是否合法、header 格式对不对。 这样分层后，错误定位非常清楚： 找不到 \\r \\r ：是分帧阶段问题（可能数据未收全或格式损坏） yyparse() 失败：是语法阶段问题（字段结构不符合规则） 性能和稳定性更好 状态机是一趟线性扫描，O(n)，几乎不分配额外内存。 相比“到处 strstr 搜索 + 多次切片拷贝”，这种方式更可控，也更适合后续处理 pipelining 和并发连接。 对于如下的示例报文： POST /login HTTP/1.1\\r Host: localhost:8888\\r Content-Length: 11\\r \\r hello=world 状态机只盯这 4 个字节序列： 读到 \\r：STATE_START - STATE_CR 接着读到 ：STATE_CR - STATE_CRLF 接着读到 \\r：STATE_CRLF - STATE_CRLFCR 接着读到 ：STATE_CRLFCR - STATE_CRLFCRLF（命中，停止扫描） 也就是寻找头部的边界，而并去未处理任何语义，也不会去处理Request Body。处理之后我们得到的变量 i 就是头部的长度！ 2.1.2. TODO 与 parser.y我们大概理解这个状态机在做什么就行，后续也不需要修改他的代码，重点是来到了 TODO ，这里是实验留给我们的任务中最关键的一步：我们要实现解析多行 Header 究竟要如何实现？ 到这里我们可能会踏足第一个误区而一头雾水：点进 yyparse() 方法试图观察是如何处理的，但是会发现进入了一个包含一千五百多行代码的文件中，根本看不懂，实际上包括它在内的几个文件 y.tab.c y.tab.h lex.yy.c ，都是编译出的的中间产物，如果我们在终端中执行 make clean 操作，他们就都不见了。那么知道了这一点，结合文件名来看，仅剩下的 parser.y 就理所当然成了我们分析的重点。（至于 lexer.l，不需要改。它的职责是把输入字符切分成 token（例如方法名、空格、冒号、CRLF、普通 token 字符等）） 2.1.3. 文法规则与推导来到 parser.y ，注释也给出了我们需要拓展的地方，我直接介绍这部分到底在完成一个什么样的事情，然后你就能明白整个文件大概是在干什么了，视角来到这个文件的 200 行： request_line: token t_sp text t_sp text t_crlf YPRINTF(request_Line: %s %s %s ,$1, $3,$5); strcpy(parsing_request-http_method, $1);\tstrcpy(parsing_request-http_uri, $3);\tstrcpy(parsing_request-http_version, $5);request_header: token ows t_colon ows text ows t_crlf YPRINTF(request_Header: %s %s ,$1,$5); strcpy(parsing_request-headers[parsing_request-header_count].header_name, $1);\tstrcpy(parsing_request-headers[parsing_request-header_count].header_value, $5);\tparsing_request-header_count++;;/* * You need to fill this rule, and you are done! You have all the assembly * needed. You may wish to define your own rules. Please read RFC 2616. * All the best! * */request: request_line request_header t_crlf\tYPRINTF(parsing_request: Matched Success. );\treturn SUCCESS;; 首先，大括号包裹的部分是一些要执行的代码，理解这部分的重点不在于此，我们重点去观察前面这种我们从未见过的部分： request_line: token t_sp text t_sp text t_crlf;request_header: token ows t_colon ows text ows t_crlf;request: request_line request_header t_crlf; 它们可以理解为“一个符号如何展开成更小的符号”，和我们学过的文法推导非常像。为了直观，我们先看一个小例子： A - B E B - C D 那么 A 就可以继续展开成： A - C D E 如果再加一条“归约规则”（把一段组合再折叠回一个新符号）： C D E - F 那我们就能得到： A - F 这就是 parser 在做的核心动作：先按规则展开匹配，再把匹配到的片段归约成一个更高层的结构。 那么就不难理解，我们实际代码中的 request, request_line, request_header 这些符号，就是我们例子中的 A, B, C，结合这些单词的含义不难得出，这些行的真实含义是这样的： 一个请求由请求行、请求头和末尾空行组成：request - request_line request_header t_crlf 一个请求行由一个符号、空格、文本、空格、文本和末尾换行组成：request_line - token t_sp text t_sp text t_crlf 一个请求头由一个符号、可选空白（187行有他的定义，用 | 连接表示它有多重的展开方式）、冒号、可选空白、文本、可选空白和末尾换行组成：request_header - token ows t_colon ows text ows t_crlf 2.1.4. 多行 Header也就是说在这里，因为 request - request_line request_header t_crlf 这一行就规定了只有一个 request_header ，如果写成 request - request_line request_header request_header t_crlf 就能解析两行了嘛，那么对于这种 header 总数量无法预知的情况来说我们该如何写呢？在这里你可以自行思考一下。 答案是这样的：我们先将 request: request_line request_header t_crlf 修改为 request: request_line request_headers t_crlf （加了个s）然后将 request_headers 定义为： request_headers: /* empty */ | request_headers request_header ; 代表着 request_headers 可以被解析为空（就不会拓展了），也可以从 request_headers 变为 request_headers request_header两个符号。 对于只有一行请求头的请求体，request_headers 先变成 request_headers request_header ，然后前面的 request_headers 变为空，最终就只剩下了一个 request_header 。 对于有三行请求头的请求体，request_headers 先变成 request_headers request_header ，然后前面的 request_headers 再变为request_headers request_header，然后再这样变一次，就变成了 request_headers request_header request_header request_header，最终 request_headers 再变为空，就剩下了三个 request_header 。 按照这种逻辑，不难得出这样做的话就可以支持任意个 request_header 出现在请求体中了。 事实上，lexer 被称之为词法分析器，它任务是把原始字符串切成一个个“词”（token），并给它们分类，比如 token、t_sp、t_colon、t_crlf、text 等。它不关心“这一整段是不是一个合法请求”，只负责把输入变成可处理的符号序列。parser 语法分析器的任务是根据我们在 parser.y 里写的文法规则（也就是上面的过程），对这些 token 序列做匹配和归约。只有当符号排列满足某条规则时，规则后面大括号里的语义动作才会执行。这二者背后都依赖成熟且复杂的编译原理算法：例如词法层常用正则到自动机（NFADFA）的构造与状态转移；语法层则有 LLLR 等体系（如 LR(1)、LALR(1)），通常通过先生成分析表（actiongoto），运行时再按“查表 + 移进归约”完成解析。该项目使用了 yacc 这样成熟的生成器去自动生成中间代码（Makefile 中定义了将 .l 和 .y 生成为中间 .c 和 .h 产物，然后我们自己的代码就可以用了），从而省略了我们自己实现这些复杂代码的过程。 所以在这一步里，我们真正完成的事情是：通过 request_headers 的递归定义，让语法层支持“任意多行 header”；而在每次匹配到 request_header 时，执行对应语义代码，把 header_nameheader_value 追加写入 Request 结构体的 headers 字段。换句话说：词法层负责“切词并分类”，语法层负责“按规则组句并落结构体”。 2.1.5. headers 扩容随后我们去拓展 request_header: token ows t_colon ows text ows t_crlf 后大括号的这一段，大括号代表着当我们的输入匹配到这一段的时候要执行怎样的代码，当前的主体代码是： strcpy(parsing_request-headers[parsing_request-header_count].header_name, $1);strcpy(parsing_request-headers[parsing_request-header_count].header_value, $5);parsing_request-header_count++; 不难看出我们每一次都给 headers 里面加入新的东西，然后让 header 的数量加一，下一次就能添加新的东西了，那么问题是我们定义的这个 headers 数组有多大？假设我存在很多很多的 header 他是否装得下呢？如果我将这个数组开得非常非常大，在高并发下是不是又消耗了太多资源呢？ 在 parse.c 中，我们发现目前的实现是： //TODO You will need to handle resizing this in parser.yrequest-headers = (Request_header *) malloc(sizeof(Request_header)*1); 也就是我们只给 headers 分配了能存放一个 Request_header 大小的空间，TODO 也提示我们了要实现 risizing ，也就是要在 parser.y 中写扩容代码，在 headers 的容量被填满的时候再为他分配额外的空间，相信在学习数据结构的时候你已经明白：（1）如何去正确实现扩容代码，（2）如何高性能实现扩容。 （1）正确实现扩容的关键是严格遵循容量增长后对内存重新分配、边界检查与元素拷贝移动的完整流程，确保数据不丢失且指针与元数据一致。 （2）高性能扩容的关键是采用合理的增长策略（如倍增）、最小化拷贝次数并优先使用连续内存和批量移动，以降低摊销成本并减少缓存失效。 标准做法： 在 parse.h 中新增一个 int header_capacity 字段用来记录容量： typedef struct\tchar http_version[50];\tchar http_method[50];\tchar http_uri[4096];\tRequest_header *headers;\tint header_count;\tint header_capacity; Request; 在 parse.c 中初始化容量： //TODO You will need to handle resizing this in parser.yrequest-header_capacity = 8;request-headers = malloc(sizeof(Request_header) * request-header_capacity); 在 parse.y 对应代码块中添加扩容代码： request_header: token ows t_colon ows text ows t_crlf YPRINTF(request_Header: %s %s ,$1,$5);\t// 动态扩容 headers 数量\tif (parsing_request-header_count = parsing_request-header_capacity) parsing_request-header_capacity *= 2; parsing_request-headers = realloc(parsing_request-headers, sizeof(Request_header) * parsing_request-header_capacity); strcpy(parsing_request-headers[parsing_request-header_count].header_name, $1);\tstrcpy(parsing_request-headers[parsing_request-header_count].header_value, $5);\tparsing_request-header_count++;; realloc 语义与用法：realloc(ptr, new_size) 会调整一块已分配内存的大小： 如果 ptr 为 NULL，等价于 malloc(new_size)。 如果 new_size 为 0，行为等价于 free(ptr)（返回值实现相关，通常为 NULL）。 可能原地扩展，也可能分配新内存并拷贝原数据；成功返回新指针，失败返回 NULL，原指针仍有效。 当然，也可以实现计算单个 Request_header 结构体的大小，在增加前计算并扩容，这样不需要新增 header_capacity 字段，但是其可维护性和性能会有损失。 到这里位置，我们再次 make 并运行 ./example samples/request_get 就能得到正确的结果了（对于这种多行 Header 的文件之前是会出错的，之前只能执行 samples\\sample_request_example ） 提示：确保你打开 samples/request_get 文件后左下角显示的是 CRLF 而非 LF ，否则需要先修改一下~ 2.2. 进程间通信example.c 是给出了一个使用 parse() 解析一个完整 HTTP 报文的过程，但是 echo_server.c 的代码里并没有真正去使用 parse() 这个函数，因此我们需要阅读其代码，明确两点：（1）原本的项目结构是如何实现与 echo_client 通信的；（2）如何在正确的位置拓展并正确使用 parse() 方法。 2.2.1. 文件系统与文件描述符在进入通信代码之前，我们先回顾两个最基础的概念：文件系统和文件描述符。这两个概念后面会频繁出现： 一、文件系统是什么？ 可以理解成：操作系统用来管理可读写对象的机制。磁盘文件是文件，终端是文件，我们用的屏幕和鼠标都是文件，网络连接在系统里也被当成一种文件来处理。所以系统提供了一套统一的接口去读写这些对象。我们可以对文件进行读写操作。 二、文件描述符是什么？ 当你打开一个文件，或者建立一个网络连接，系统会返回一个整数编号。这个编号就是文件描述符，它的作用很简单：告诉系统你要读写哪个对象。因此你会看到很多函数都是 read(fd, ...)、write(fd, ...)、close(fd)——它们读写的对象其实就是 fd 指向的那个东西。 在代码中，socket 连接本质上也是一个文件描述符，只是它代表的是一条网络连接，操作系统将一个 socket 抽象为一个文件，我们对它进行读写就可以，至于如何发送以及各自机制都是由底层来实现的。 有了这个最基础的认识，我们再来看项目里的进程间通信。 echo_client：客户端，用来发消息 echo_server：服务器，用来收消息并回发 目前它们的关系非常简单：echo_client 连上 echo_server，发出一段字符串，服务器再把这段字符串原样回给它。 2.2.2. 通信简历流程一、服务器 echo_server.c 在做什么？ 服务器是标准的 TCP 服务器流程： socket() 创建监听端口 sock = socket(PF_INET, SOCK_STREAM, 0); 参数中：PF_INET 表示 IPv4 协议族（和 AF_INET 等价），SOCK_STREAM 表示 TCP（面向连接的字节流），0 表示使用默认协议（在这里就是 TCP）。这些常量在 socket.h 和 in.h 里定义。 返回值是文件描述符，成功时是非负整数，失败返回 -1。 bind() 绑定端口号 bind(sock, (struct sockaddr *) addr, sizeof(addr)); addr 是 struct sockaddr_in（定义在 in.h），我们在前面把它的字段填好了： addr.sin_family = AF_INET; // 指定 IPv4addr.sin_port = htons(ECHO_PORT); // 指定端口号（网络字节序）addr.sin_addr.s_addr = INADDR_ANY; // 表示监听所有本机网卡地址 bind 成功返回 0，失败返回 -1。 listen() 开始监听 listen(sock, 5); 5 是等待连接的队列长度（backlog），表示最多允许多少个未处理的连接排队（全连接和半连接的个数，回顾一下三次握手）。 listen() 的作用是让内核开始为这个 socket 建立连接等待队列（backlog）。一旦 echo_client 发起连接，accept() 返回一个新的文件描述符，echo_server 程序进入处理逻辑。由于目前 echo_client 是单线程、顺序处理的服务器，当你正在处理一个连接时，新的连接会先排队等待（队列满了才会失败），所以你只能一个一个处理（进入了一个循环）。 listen 成功返回 0，失败返回 -1。 accept() 等待客户端连接 client_sock = accept(sock, (struct sockaddr *) cli_addr, cli_size); 会在这里阻塞等待，直到有客户端连上来才继续向下执行；它返回的是一个新的文件描述符，专门代表这一次建立好的连接（从上文可知：这个连接是一个文件，我们操作它的文件描述符就可实现读写了）。从函数的参数可以明确：它从 sock 文件描述符在上文中建立的队列中取一个连接，然后按照 cli_addr 的格式接收（它是 sockaddr_in 类型的，你可以完全参考上文 bind 中对 addr 的定义理解对它的定义，只不过这里是 accept 函数返回一个结构体供我们接收）。注意：sock 只是监听用的入口，真正读写数据用的是 client_sock。（前一个可以理解为我们自己的端口（收到连接后它会存到一个队列里，由内核维护），后面的才是真正的连接（也就是从 echo_client 来的请求连接），它们现在都是文件） recv() 读取客户端数据 readret = recv(client_sock, buf, BUF_SIZE, 0); client_sock：要读的连接，也就是 accept() 返回的文件描述符。 buf：接收缓冲区的起始地址，数据会写到这里。 BUF_SIZE：最多读多少字节（缓冲区大小），防止写越界。 0：标志位，用默认行为即可（这里没有特殊需求）。 send() 把数据回发 send(client_sock, buf, readret, 0); client_sock：要写的连接 buf：发送缓冲区的起始地址（要发的数据） readret：要发送的字节数，通常就是刚刚 recv 到的长度 0：标志位，默认发送 二、客户端 echo_client.c 在做什么？ 客户端逻辑非常短，只做三件事： 连接服务器：connect() 也就是我们 accept() 接收的东西，如果你能理解上述的内容一定可以理解这一点的，我们就不多做介绍了。 发送数据：send() 接收回显：recv() 所以它只是一个“把输入发出去”的 TCP 客户端，它并不理解 HTTP。 也就是说：**通信已经完成了，但业务逻辑还没有开始。**它现在只是一个回声服务器。 3. 代码拓展那么接下来的任务就十分明确了：如何拓展 echo_server.c 的功能，将 parser() 方法添加到合适的位置，将通过 TCP 连接传入 server 中的 HTTP 报文 进行处理。如果我们确实读懂了代码的话就清楚，我们添加的思路大概是这样的： while((readret = recv(client_sock, buf, BUF_SIZE, 0)) = 1) // 在这里调用 parse ，得到一个 *Request 但是在这里存在几个关键问题： recv() 读到的是字节流，不保证一次就是一个完整 HTTP 请求。 一次 recv() 可能读到多个请求（粘包，如果连续发送了两个连接，读到的东西可能是上一个请求的尾巴和下一个请求开始的一部分），不能只解析一次就结束。 如果请求不完整，应该继续读取；如果格式错误，应该立即返回 400。 parse() 成功后，还要根据方法决定是回 200 还是 501。 处理完一个请求后，缓冲区里可能还有后续请求，必须“消费已处理字节”再继续解析。 send() 可能短写，响应发送需要完整写出。 HEAD 方法只返回响应头，不返回 body。 缓冲区被占满却仍无法形成合法请求时，应返回 400 并关闭连接。 实际上处理这些问题的办法有很多，下面是我的实现思路仅供参考： 3.1. 解析接口改造最初版本里，parse 的签名是： Request * parse(char *buffer, int size, int socketFd); 这个设计有一个核心问题：它只有“返回 Request* 或 NULL”两种结果。 但在 TCP 字节流场景里，NULL 其实可能代表三种完全不同的情况： 当前数据只是不完整（应该继续 recv） 报文格式错误（应该返回 400 Bad request） 内存分配失败等系统错误（不应按业务错误处理） 如果这三种都混在一个 NULL 里，echo_server 就无法写出正确分支。 因此我们把接口改成了结果状态 + 输出参数模式： ParseResult parse(char *buffer, int size, Request **out_request, size_t *out_consumed) 我删除了 socketFd 参数，因为我认为这不是一个好的处理方式：真正利用 socketFd，让 parse 在 PARSE_INCOMPLETE 时内部继续 recv 会使 main 函数会更短，但 parse 和网络 IO 强耦合，后面 week4 做 select 的重构会更痛苦。 并新增枚举： typedef enum\tPARSE_OK = 0,\tPARSE_INCOMPLETE,\tPARSE_BAD_REQUEST,\tPARSE_OOM ParseResult;ParseResult parse(char *buffer, int size, Request **out_request, size_t *out_consumed);// 记得在这里修改 `parse` 函数的定义 这样 parse() 的语义就清晰了： PARSE_INCOMPLETE：数据未收全，server 继续读； PARSE_BAD_REQUEST：报文错误，server 回 400； PARSE_OK：成功解析，out_request 拿到结构体； out_consumed：告诉 server 这次消费了多少字节（用于处理粘包多请求）。 这一步是后续改造 echo_server 的前提：**先把解析器的“状态表达能力”补齐，再把它接入网络收包主循环。**后续如果需要拓展，只需要在拓展 enum 后在 parse() 中添加分支，然后在主函数中进行处理。 3.2. body 完整性判断再观察现在的代码结构：到这里有一个非常容易误解的点：yyparse() 返回 SUCCESS，只代表请求行 + 首部字段的语法匹配成功，并不代表整个 HTTP 请求已经完整到达。 也就是说，语法层只告诉我们“头部文本长得对”；但对于带请求体的报文（例如 POST），我们还必须回答另一个问题：**body 收全了吗？**接下来完整的代码修改是这样的： //Valid End Stateif (state == STATE_CRLFCRLF) size_t header_len = (size_t)i; //前文说过，这里就是 header 部分的长度呢 Request *request = (Request *) malloc(sizeof(Request)); if (!request) return PARSE_OOM; request-header_count = 0; //TODO You will need to handle resizing this in parser.y request-header_capacity = 8; request-headers = malloc(sizeof(Request_header) * request-header_capacity); if (!request-headers) free(request); return PARSE_OOM; set_parsing_options(buf, i, request); if (yyparse() == SUCCESS) size_t content_len = 0; if (get_content_length(request, content_len) 0) free(request-headers); free(request); return PARSE_BAD_REQUEST; if ((size_t)size header_len + content_len) free(request-headers); free(request); return PARSE_INCOMPLETE; if (out_request) *out_request = request; if (out_consumed) *out_consumed = header_len + content_len; return PARSE_OK; free(request-headers); free(request); return PARSE_BAD_REQUEST;//TODO Handle Malformed Requestsreturn PARSE_INCOMPLETE; 其中 get_content_length 方法的定义如下： static int get_content_length(const Request *req, size_t *out_len)\tint i;\t*out_len = 0;\tfor (i = 0; i req-header_count; i++) if (strcasecmp(req-headers[i].header_name, Content-Length) == 0) char *end = NULL; long val = strtol(req-headers[i].header_value, end, 10); if (end == req-headers[i].header_value || val 0) return -1; while (*end == || *end == \\t) end++; if (*end != \\0) return -1; *out_len = (size_t)val; return 0; return 0; 可以按 4 步理解： 解析并校验 Content-Length get_content_length() 会在 headers 中查找并解析 Content-Length。如果字段值不是合法非负整数（例如有脏字符、负数），直接判定为 PARSE_BAD_REQUEST 。 计算“完整请求”应有的总长度 完整长度 = header_len + content_len 其中 header_len 是 \\r \\r 结束位置，content_len 是请求体长度。 判断当前缓冲区是否收全 如果 size header_len + content_len ，说明这次 recv 只拿到了部分 body，这时不能报 400，而应返回 PARSE_INCOMPLETE ，让上层继续读。 仅在语法正确 + 数据完整时返回成功 设置 out_request 和 out_consumed，再返回 PARSE_OK 。这样 server 才能安全地处理本次请求，并知道应当从缓冲区消费多少字节。(不知道我们处理了多少字节的话，粘包一旦发生就不知道哪里是上一个包的末尾，哪里是下一个包的开始了) 3.3. 主循环与粘包处理接下来我们就可以编写 echo_server.c 的主循环了，接下来我先给出完整的代码： while (!close_conn (readret = recv(client_sock, buf + used, sizeof(buf) - used, 0)) 0) used += (size_t)readret; for (;;) Request *request = NULL; size_t consumed = 0; ParseResult pres = parse(buf, (int)used, client_sock, request, consumed); if (pres == PARSE_INCOMPLETE) break; if (pres != PARSE_OK) const char *bad = HTTP/1.1 400 Bad request\\r \\r ; send_all(client_sock, bad, strlen(bad)); close_conn = 1; break; if (!is_supported_method(request-http_method)) const char *not_impl = HTTP/1.1 501 Not Implemented\\r \\r ; send_all(client_sock, not_impl, strlen(not_impl)); else char resp_hdr[256]; int hdr_len = snprintf(resp_hdr, sizeof(resp_hdr), HTTP/1.1 200 OK\\r Content-Length: %zu\\r \\r , consumed); send_all(client_sock, resp_hdr, (size_t)hdr_len); if (strcmp(request-http_method, HEAD) != 0) send_all(client_sock, buf, consumed); free(request-headers); free(request); if (consumed used) used = 0; break; memmove(buf, buf + consumed, used - consumed); used -= consumed; if (used == 0) break; if (used == sizeof(buf)) const char *bad = HTTP/1.1 400 Bad request\\r \\r ; send_all(client_sock, bad, strlen(bad)); close_conn = 1; break; close_socket(client_sock); 其中 close_socket 和 send_all 的定义如下： int close_socket(int sock) if (close(sock)) fprintf(stderr, Failed closing socket. ); return 1; return 0;static int send_all(int sock, const char *buf, size_t len) size_t sent = 0; while (sent len) ssize_t n = send(sock, buf + sent, len - sent, 0); if (n = 0) return -1; sent += (size_t)n; return 0; close_socket：统一关闭 socket 的行为和错误处理。 如果每个分支都手写 close()，很容易漏掉错误检查，或者在多处逻辑里写出不一致的收尾代码。封装后可读性和一致性都更好。 send_all：保证“把该发的数据完整发出去”。 send() 在 TCP 下并不保证一次就把 len 字节全部发送完成，可能出现短写。如果我们只调用一次 send()，就可能只发出半个响应头，客户端看到的就是残缺报文。send_all 用循环把剩余字节继续发送，直到全部发完或明确失败。 所以这两个函数本质上是：把重复且容易出错的系统调用细节收敛起来，让主循环只关注协议流程。 3.4. 主循环条件与控制流主循环条件是： while (!close_conn (readret = recv(client_sock, buf + used, sizeof(buf) - used, 0)) 0)\t... 这个条件本身就编码了三层语义： !close_conn：业务上还允许继续处理这个连接。 一旦我们已经判定要关闭（比如发送了 400），就不应继续读。 buf + used：新数据追加到“未处理残留”后面。 这是为了解决半包：上次可能只收到一半请求，不能覆盖旧数据。 sizeof(buf) - used：只使用缓冲区剩余空间。 防止写越界，同时把“空间是否耗尽”纳入控制流（后面有 used sizeof(buf) 的保护分支）。 然后每轮 recv 之后，内层 for (;;) 做的是尽可能多地从当前缓存里拆请求（每层 while 的开始都会读一些内容到 buf 中，在 for 中 break 就意味着继续读）： parse == PARSE_INCOMPLETE：当前数据还不够，跳出内层，回外层继续 recv。 parse != PARSE_OK：格式错误，发 400，标记关闭连接。 parse == PARSE_OK：根据方法返回 200 或 501，释放请求对象，然后按 consumed 前移缓冲区继续解析下一个请求。 这就是一次收包，可能解析多个请求的核心能力，也是处理粘包。后续就是处理解析后的请求：按照要求向客户端回消息就可以了。 4. 编译运行与实验结果4.1. 基础设施3：Makefile如果这时候运行 make，很可能会出现“代码明明没语法问题，但还是编不过”的情况。这类错误通常不是 echo_server.c 本身写错，而是 Makefile 的链接规则不完整。 4.1.1. 问题在哪里：只链接了 echo_server.o先看原本的写法（注意：命令必须换行并用 Tab 缩进，不能写在同一行）： echo_server: $(OBJ_DIR)/echo_server.o\t$(CC) -Werror $^ -o $@ 这条规则只把 echo_server.o 链接成可执行文件，但现在我们在 echo_server.c 实际调用了 parse()，因此需要修改，那么原本使用了 parse() 方法的 example.c 如何在 Makefile 中写的就很有参考意义。 parse() 的实现并不在 echo_server.o，而在： parse.o（parse.c 编译出来的） 以及解析器依赖的 y.tab.o、lex.yy.o（由 yacc/flex 生成的 .c 再编译出来的） 因此只链接一个 echo_server.o，就会出现 undefined reference 这种典型链接错误。 4.1.2. 编译 vs 链接这里先复习一下两个概念： 编译（compile）：*.c - *.o，每个源文件独立生成目标文件。 链接（link）：把多个 .o 合并成最终可执行文件。 #include parse.h 只提供函数声明，并不会把函数实现自动带进来；实现是否存在，取决于链接阶段有没有把对应 .o 放进去。因此我们的任务就是把函数实现放进去！ 4.1.3. 正确做法参考 example 的实现形式，对于 echo_server ，我们把解析器相关的 .o 抽成一组公共对象： 参考语法： $@：当前目标名（例如 echo_server） $^：当前规则的全部依赖（展开后是一串 .o） $：第一个依赖（通常用在“一对一编译规则”里） # parser-related objects shared by example and echo_serverPARSER_OBJ := $(OBJ_DIR)/y.tab.o $(OBJ_DIR)/lex.yy.o $(OBJ_DIR)/parse.o# per-binary objectsEXAMPLE_OBJ := $(OBJ_DIR)/example.o $(PARSER_OBJ)SERVER_OBJ := $(OBJ_DIR)/echo_server.o $(PARSER_OBJ)CLIENT_OBJ := $(OBJ_DIR)/echo_client.o 然后分别写三个可执行文件的链接规则： example: $(EXAMPLE_OBJ)\t$(CC) $^ -o $@echo_server: $(SERVER_OBJ)\t$(CC) -Werror $^ -o $@echo_client: $(CLIENT_OBJ)\t$(CC) -Werror $^ -o $@ 这样一来： echo_server 需要什么就链接什么：echo_server.o + parse.o + y.tab.o + lex.yy.o example 同样依赖同一套解析器对象文件 后续你继续改 parse.c / parser.y / lexer.l，不会再出现“example 能编，server 不能编”的分裂状态 当你执行 make echo_server，最终等价于组织出类似这样的链接命令（顺序由依赖决定，但核心是链接阶段把需要的 .o 全放进去）： gcc -Werror obj/echo_server.o obj/y.tab.o obj/lex.yy.o obj/parse.o -o echo_server 到这里 Makefile 的问题就解决了：编译阶段各编各的，链接阶段把该拼的拼齐。 4.2. 展示结果首先make clean， make，然后 ./echo_server 运行我们的服务端，随后在本机浏览器（我这里是 Chrome ）中输入 http://127.0.0.1:8888/ （避免走 HTTPS 方法，需要完整输入） 浏览器中得到： 到此，我们第一周的任务就完成了。 4.3. 协议设计（目前阶段） 请求消息解析方法parse() 实现分帧 + 语法 + 完整性校验。先用状态机扫描 \\r \\r 找到头部结束位置（header_len），再把头部交给 yyparse() 做语法归约（方法URI版本headers 落入 Request 结构体），最后读取并校验 Content-Length。若 size header_len + content_len 返回 PARSE_INCOMPLETE ；语法或长度非法返回 PARSE_BAD_REQUEST ；只有语法正确且数据完整才返回 PARSE_OK，并通过 out_consumed 告知本次消费字节数。 接收缓冲区设计服务端使用固定接收缓冲区（BUF_SIZE = MAX_HEADER_SIZE），used 记录已占用字节。每次 recv 追加到 buf + used，随后循环调用 parse() 尽可能多地解析请求。成功后通过 memmove(buf, buf + consumed, used - consumed) 前移剩余数据，实现半包保留与粘包拆分；PARSE_INCOMPLETE 则继续收包；缓冲区打满仍无法推进时返回 400 并关闭连接。（你也可以在第一周去掉有关粘包的处理） 日志记录模块设计当前版本没有独立日志模块，采用最简的方法：启动信息走 stdout，错误走 stderr（如 socket/bind/listen/accept/close 失败）。close_socket() 统一了关闭错误输出。该设计符合第一周先跑通协议流程的目标，不包含任何复杂功能（这在第二周实现）。 其它设计细节方法白名单为 GETHEADPOST，其余方法返回 501；格式错误返回 400。HEAD 仅返回响应头不返回 body。发送端使用 send_all() 处理短写，保证响应完整写出。每次解析完成后释放 request 与 request-headers，避免内存泄漏。整体为单进程单线程阻塞模型，重点验证 HTTP 解析与响应语义，尚未引入多路复用（第四周的任务）。","tags":["network","socket","liso","http"]},{"title":"“一劳永逸” 的开发容器入门 ：Docker 容器 + VS Code 全流程","path":"/blog/2026/02/04/“一劳永逸” 的开发容器入门 ：Docker 容器 + VS Code 全流程/","content":"0. 写在前面这篇文章的目标很简单：**带我们用“开发容器”把开发环境固定下来，让项目在不同电脑上也能稳定复现。**并不是 Docker 专业教程。个人对 Docker 相关概念的掌握程度也只是够用为止，这里我们更关注两件事： 我们要怎么把容器跑起来、并且长期用得顺手 我们要怎么把配置写进仓库，让同学同事照着就能复现同一套环境 我要说的是如何用开发容器解决环境问题，而不是如何精通 Docker。 0.1 读者基础这篇文章适合： 想要一个可复现的开发环境，但对 Docker 不熟悉 需要团队协作，或者经常换电脑系统 希望用 VS Code 直接连接容器开发 我们需要的基础不多： 能使用终端执行命令（Windows 的 PowerShellCMD 或 macOSLinux 终端均可） 了解最基本的 Linux 常识（比如 ls / cd / pwd 这些命令即可） 0.2 目标我们会得到一套能直接复用的工作流： 用 DockerfileCompose 搭出一个能长期使用的开发容器 学会在终端进入容器、执行命令、验证环境是否正确 学会用挂载把本地代码与容器同步（开发体验像在本机一样） 学会用 VS Code 一键连接容器，把编辑终端调试统一到容器里 遇到常见问题（权限、同步、容器找不到等）知道该从哪里排查 1. 使用开发容器进行开发的好处在实际的作业乃至工作过程中，我们经常会遇到一些让人摸不着头脑的问题： 为什么明明按照别人的步骤一步不落地操作，我却做不出相同的效果？为什么这份代码在我电脑上能正常运行，换到别人那里就无法复现？ 当然，很多时候只要肯花时间查资料、定位报错，我们大概率还是能把问题解决掉。但现实是：这种“为了让环境跑起来”而投入的时间、人力成本往往不成比例。更麻烦的是，为了兼容某个运行环境，我们可能不得不新增、更换甚至卸载一些工具或依赖，而这类改动又很容易引发新的问题，最后变成“修 A 出 B，修 B 出 C”的连锁反应。 因此我们希望尽可能屏蔽由操作系统差异、依赖版本不一致带来的影响，把开发环境本身也固定下来、记录下来、能复现出来——这就是开发容器要解决的事情。 你的代码在仓库里，容器配置（Dockerfile Compose devcontainer.json）也在仓库里。别人拉下仓库后，照着跑就能得到同样的开发环境。 另外，从个人角度来说，在一个 Linux 环境的开发容器中完成开发，也能顺带提升我们对 Linux 环境与常用工具链的熟练程度。即使当前项目在 Windows 上也“看起来没问题”，我也更倾向于优先搭建开发容器并连接进行开发：一方面减少环境不确定性，另一方面也算是为将来可能的部署迁移提前做准备，做到“备不时之需”。 1.1 传统开发环境的痛点在没有容器之前，开发环境通常靠“口头传承 + 每个人自己装”： 你装的是 gcc 11，队友装的是 gcc 9，编译选项行为有差异 你在 Ubuntu 上开发，队友在 Windows 上开发，依赖安装方式完全不同 项目需要一堆库：openssl、zlib、boost……新人安装到一半就卡住 项目过几个月再回来看：忘了当时装了哪些依赖、版本是多少 这些问题的共同点是：开发环境是“漂浮的”，没有被明确记录并且可复现。 所以你经常会看到下面这种对话： “你编译报错？我这里没问题啊。” “你装的是什么版本？我也不知道，我之前装过一次。” 开发容器的目标就是：让这种对话尽量消失。 1.2 开发容器带来的核心价值你可以把开发容器理解成一件很朴素的事：把开发环境也当成项目的一部分保存下来。它带来的收益十分明显：更省心、更稳定。 （1）环境一致在团队协作里，最容易踩雷的并不是代码本身，而是每个人的环境都不太一样。 A 同学的编译器版本更高一点，B 同学少装了一个库，C 同学的系统发行版不同——最后问题表现出来就像随机故障。 开发容器的做法很直接：把“系统 + 工具链 + 依赖”统一写进 Dockerfile（或相关配置）里。比如你做 C++ 项目，需要 gcc/g++、cmake、gdb 以及一些常用库，那么就把它们明确写出来： 装哪些包 怎么安装 默认在哪个目录工作（例如 /workspace） 这样每个人进入容器看到的都是同一套环境，很多“只在我这里出问题”的诡异现象会明显减少。 （2）可复现对我们来说，拉代码写业务并不可怕，最可怕的是搭环境 + 解决一堆报错。而开发容器的价值就在于：把搭环境这件事从教程式手工劳动，变成可重复执行的固定流程。 通常流程会变得很简单： 拉代码 构建并启动容器 进入容器开始开发 后面我们会看到的三条命令，本质上就是把复杂的安装过程固化下来： docker compose build：按 Dockerfile 把开发环境“做出来”（生成镜像） docker compose up -d：把环境跑起来（启动容器） docker compose exec ... bash：进入容器（开始敲命令干活） 你不需要记住到底要装哪些依赖、顺序是什么，因为这些都已经被写在配置里了。 （3）可迁移很多人第一次真切体会到开发容器的好处，是在换电脑重装系统换到服务器那一刻。以前你可能要重新装一遍工具链、重新配一遍环境变量、重新踩一遍坑；而现在只要 Docker 能跑，你就可以认为“环境也能跑”。 这对下面这些场景尤其友好： 你在实验室电脑、个人电脑、服务器之间来回切换 课程大作业毕设多人协作，需要保证大家一致 需要快速在一台新机器上把环境准备好 1.3 基本概念1.3.1 镜像（Image）——环境的模版安装包镜像可以理解成一个“准备好的开发环境模版”，里面通常包含： 一个基础系统（比如 Debian Ubuntu） 安装好的开发工具（gcc、cmake、gdb、git……） 以及你在 Dockerfile 里写的所有配置 你可以把它粗略类比成“装好软件的系统快照”，但它比传统虚拟机更轻量，也更容易分发。 1.3.2 容器（Container）——镜像跑起来后的“正在工作的环境”镜像是静态的，容器是动态的：镜像放在那里不动，容器才是真正运行起来的东西。 镜像像安装包，容器像安装包运行后的程序。 你进入终端、敲 cmake、跑 make、用 gdb 调试，实际都是在容器里完成的。 1.3.3 挂载（Mount）——让容器“看见”你本地的项目目录容器有自己的文件系统，但我们写代码一般还是在本地的项目目录里管理（比如 Git 仓库就在本地）。 所以我们会做一件非常关键的事： 把本地项目目录挂载到容器的 /workspace。 这样就实现了同步效果： 你在本地改代码，容器里立刻能看到 你在容器里编译生成文件，本地目录也会出现 也正因为这个“挂载”，开发容器才会用起来像本地开发一样顺手——只是工具链换到了容器里。 1.4 开发容器 vs 本地运行我们可能会纠结：我装在本机不也能用吗？确实能，但两者的取舍点不一样。 通常比较适合用开发容器的情况是： 项目依赖多、工具链复杂，版本还比较敏感（CC++、多语言工程等） 需要多人协作，希望大家环境一致 你经常换设备，或者希望环境能随项目迁移 而下面这些情况，要么不一定需要容器，要么需要额外配置才能舒服： 强依赖宿主硬件驱动外设的开发（某些 USB 调试、部分 GPU 场景等） 项目很小、纯脚本为主，装环境成本本来就很低 不过从“写一篇能交接的教程”的角度来说，开发容器非常适合作为主线：它能把环境问题讲清楚、讲完整，也更接近真实实践的协作方式。 1.5 内容后面的主要内容是描述一套非常固定的工作流： 在项目里保存开发容器配置（Dockerfile docker-compose.yml devcontainer.json） 在任何一台能跑 Docker 的机器上： 用 Docker 构建并启动开发容器 或者用 VS Code 一键 “Reopen in Container” 进入容器后，你就处在一个“统一、可复现”的开发环境里： 编译、运行、调试都在容器里完成 代码和本地目录同步（通过挂载） 需要什么依赖就继续往配置里加，逐步完善这套环境 我们后面做的每一步，最终都是在服务三件事——统一环境、保证同步、尽量开箱即用。 2. 搭建开发容器环境我们已经明确：开发容器的目的，是把开发环境固定下来并且可复现。 先准备 Docker（让容器能启动） 再写 Dockerfile（把工具链装进镜像） 再用 Compose（把启动参数写成配置，方便反复使用） 最后用几条命令验证：容器能进、代码能同步、工具能用 2.1 Docker Desktop Docker Engine开发容器的前提只有一个：本机能正常运行 Docker。 2.1.1 我们需要准备什么 Windows macOS：一般使用 Docker Desktop Linux：通常安装 Docker Engine（并确保能使用 docker 与 docker compose） 后面的命令都默认在终端执行：Windows 的 PowerShell CMD、macOSLinux 的 Terminal、或 VS Code 的集成终端都可以。 2.1.2 用最简单的方式验证 Docker 是否可用在终端执行： docker versiondocker compose version 预期输出： docker version 能输出 Client Server 信息 docker compose version 能输出 Compose 版本号 如果其中任意一个报错，表示我们未能成功配置环境。 2.2 Dockerfile 编写：把开发工具链做成“可复用的环境模版”Dockerfile 是开发容器的“配方”。我们希望它做到两件事： 明确写出我们需要哪些工具与依赖 任何人用它构建出来，得到的环境都一致 这一节我们先从“最小可用”开始，再补充一些常见的工程化写法。 2.2.1 一个最小可用的 Dockerfile在项目根目录新建 Dockerfile： FROM debian:bookworm-slimENV DEBIAN_FRONTEND=noninteractiveRUN apt-get update \\ apt-get install -y --no-install-recommends \\ build-essential cmake gdb git \\ rm -rf /var/lib/apt/lists/*WORKDIR /workspace 这份 Dockerfile 做的事情很简单： 选择一个基础 Linux（Debian）(我们也可以选择Ubuntu镜像) 安装 CC++ 开发常用工具（编译、构建、调试、Git） 把默认工作目录设为 /workspace（后面我们会把本地项目挂载到这里） 到这里为止，我们已经拥有了一个“能编译、能调试、能拉代码”的基础环境。 2.2.2 为什么要 rm -rf /var/lib/apt/lists/*这是一个常见的镜像体积优化：apt-get update 会下载索引文件，如果不清理，镜像会变大。清理后不影响运行，只影响后续再次 apt-get install 之前需要重新 apt-get update。 2.2.3 一个更偏工程的 Dockerfile当我们把容器当“日常开发环境”使用时，会遇到一个很典型的问题：挂载目录的权限（尤其在 LinuxmacOS 上更明显）：容器里用 root 创建的文件，本地可能不好改；或者本地文件在容器里写不了。 一个常见做法是：在镜像里创建普通用户，并尽量对齐 UIDGID： FROM debian:bookworm-slimENV DEBIAN_FRONTEND=noninteractiveARG USERNAME=vscodeARG USER_UID=1000ARG USER_GID=1000RUN apt-get update \\ apt-get install -y --no-install-recommends \\ build-essential cmake gdb git \\ rm -rf /var/lib/apt/lists/*RUN groupadd --gid $USER_GID $USERNAME \\ useradd --uid $USER_UID --gid $USER_GID -m -s /bin/bash $USERNAMEWORKDIR /workspaceUSER $USERNAME 可以记住：在容器里用普通用户开发，通常比一直用 root 更省心。 2.3 按需配置 Docker Compose：把“启动参数”写进文件里Dockerfile 解决了“环境里装什么”，Compose 解决的是“容器怎么启动”。而我们最容易忽略的一点是：仅仅有 Dockerfile 并不够，我们还需要决定： 把本地哪个目录挂载到容器哪里 容器启动后默认在哪个目录 容器要不要一直运行（方便随时进入） 未来要不要加数据库缓存等服务（在本机将这些服务作为镜像运行是非常美妙的） Compose 的价值就是：把这些启动参数写进 docker-compose.yml，以后不需要重复手敲长命令。 2.3.1 最简 compose在项目根目录新建 docker-compose.yml： services: dev: build: . container_name: dev working_dir: /workspace volumes: - ./:/workspace tty: true stdin_open: true command: sleep infinity 我们只需要理解： build: .：使用当前目录的 Dockerfile 构建镜像 volumes: ./:/workspace：把本地项目目录同步到容器 /workspace command: sleep infinity：让容器一直运行，便于我们随时进入 2.3.2 用 Compose 把环境跑起来在项目目录执行： docker compose builddocker compose up -d 这两条命令对应两件事： build：按 Dockerfile 做出镜像（环境模版） up -d：启动容器（并在后台运行） 2.3.3 进入容器docker compose exec dev bash 进入后我们可以做三个最直观的验证： pwdlsgcc --version 预期输出： pwd 输出 /workspace ls 能看到本地项目文件 gcc --version 有正常版本号输出 如果这三步都成立，说明我们已经完成了开发容器的最小可运行版本：容器能启动、能进入、能看到本地代码、工具链可用。 2.4 命令速查 构建镜像：docker compose build 启动容器：docker compose up -d 进入容器：docker compose exec dev bash 查看日志：docker compose logs -f 停止并清理：docker compose down 2.5 常见问题2.5.1 docker pull docker compose build 很慢或超时典型表现： 拉基础镜像卡在 pulling fs layer、waiting 很久 直接报超时：i/o timeout、TLS handshake timeout、context deadline exceeded （1）先确认问题不在 Docker 本身： docker versiondocker compose versiondocker info （2）配置 Registry 镜像源 Linux WSL2（Docker Engine）：编辑 /etc/docker/daemon.json Docker Desktop（WindowsmacOS）：在 Settings 里配置（不同版本入口略有差异，一般在 Docker Engine 或 Registry mirrors） daemon.json 示例（把 mirror 地址换成我们实际要用的）： registry-mirrors: [ https://your-mirror ], dns: [8.8.8.8, 1.1.1.1] 改完后重启 Docker，再试： docker pull debian:bookworm-slim 注意：镜像源不是“越多越好”，稳定可用最重要；DNS 也经常是导致超时的隐含因素，所以这里顺手给了一个常用配置。 2.5.2 配了镜像源后继续失败常见原因：如果确定镜像源是稳定可访问的，一般就是开了梯子没关，关了就行。如果仍然无法解决请参考其他在这方面更为详细的文档。 2.5.3 apt-get update 卡住、很慢、或偶发超时典型表现： Dockerfile 构建卡在 apt-get update 很久 或反复报错下载失败 常见处理思路： 换更稳定的 apt 源 强制 IPv4（ Acquire::ForceIPv4 true;） 尽量把 apt-get update 和 apt-get install 写在同一个 RUN 我们在 Dockerfile 里常见的稳定写法就是： RUN apt-get update \\ apt-get install -y --no-install-recommends ... \\ rm -rf /var/lib/apt/lists/* 也有一定可能是开代理导致 apt 走代理而失败，还是记得要关闭梯子试一试。 3. VS Code 连接开发容器到目前为止，我们已经把开发容器“跑起来”了：容器能启动、能进入、能看到本地代码、工具链也能用。但如果每次都要手动 docker compose exec ... bash，并且编辑器还在本机、编译调试在容器里来回切换，就仿佛回到了挪威的原始森林 体验仍然十分割裂。 这一章我们要做的事情很明确：让 VS Code 直接连接到容器里工作。从此以后： VS Code 的终端就是容器终端 CC++ 等语言服务、调试器都运行在容器里 代码仍然是本地目录（通过挂载同步），我们不需要改变原来的 Git 工作流 3.1 预期效果完成连接后，我们期望看到的体验是： VS Code 里能一键“进入容器环境” Terminal 打开就是容器的 bash 运行 gcc --version、cmake --version 等命令，得到的是容器内的版本 工作区文件仍然是我们本地的项目目录，并且容器能实时同步 如果我们能做到这些，就说明这条链路已经打通了。 3.2 插件安装VS Code 连接开发容器依赖一个扩展：Dev Containers。 它的工作方式可以用一句话概括： 我们在项目里放一个配置文件，告诉 VS Code：这个项目应该用哪个容器打开；VS Code 会负责构建、启动并连接进去。 这个配置文件通常放在项目根目录的 .devcontainer/ 下面，名字叫 devcontainer.json。 3.3 Dockerfile Compose连接开发容器大体有两种常见写法： Dockerfile 路线：项目只有一个开发容器（最常见，也最适合新手） Compose 路线：除了开发容器，还需要数据库Redis 等多个服务（更像真实工程） 3.4 Dockerfile3.4.1 创建目录与配置文件在项目根目录创建： .devcontainer/devcontainer.json 填入下面这份最小配置： name: devcontainer-demo, build: dockerfile: ../Dockerfile , workspaceFolder: /workspace 这份配置我们只需要理解三个点： name：这个开发容器在 VS Code 里显示的名字 build.dockerfile：告诉 VS Code 用哪个 Dockerfile 构建镜像 workspaceFolder：告诉 VS Code 连接后默认打开哪个目录（我习惯统一用 /workspace） 注意：这里的路径写成 ../Dockerfile，是因为 devcontainer.json 在 .devcontainer/ 目录下，不能忘记返回。 3.4.2 进入容器（VS Code 操作）接下来在 VS Code 里： 打开项目目录 打开命令面板（Command Palette） 搜索并选择：Dev Containers: Reopen in Container VS Code 会开始： 构建镜像 启动容器 把当前工作区挂载进去并连接 当窗口左下角出现“Dev Container”相关标识时，通常就意味着连接成功了。 3.4.3 验证我们真的在容器里打开 VS Code 的终端（Terminal），输入： pwdgcc --version 我们期待看到： pwd 输出 /workspace（或它的子目录） gcc --version 输出的是容器内的版本信息 同时，随便打开一个源代码文件编辑并保存，再到终端 ls 看看文件是否变化——这能进一步确认挂载同步是通的。 3.5 Compose如果我们已经写了 docker-compose.yml，并且服务名叫 dev，那么 VS Code 也可以直接连接这个 service，而不是再“自己启动一套”。 3.5.1 devcontainer.json（Compose 版）同样在 .devcontainer/devcontainer.json 中写： name: devcontainer-compose, dockerComposeFile: ../docker-compose.yml, service: dev, workspaceFolder: /workspace 这里的关键点是： dockerComposeFile：告诉 VS Code 使用哪个 compose 文件 service：告诉 VS Code 我们要连接的是哪一个服务（必须和 compose 里的服务名一致） workspaceFolder：连接后默认打开的目录 然后一样使用 Dev Containers: Reopen in Container 进入。 如果 VS Code 报 “找不到 service”，几乎都是 service 名写错了：compose 里叫什么，这里就必须叫什么。 3.6 更开箱即用的配置跑通之后，如果我们希望：进入容器时自动把常用扩展装好、把默认 shell工具准备好，这样别人拉仓库就能直接用，那么： 3.6.1 自动安装 VS Code 扩展在 devcontainer.json 加一段 customizations： name: devcontainer-demo, build: dockerfile: ../Dockerfile , workspaceFolder: /workspace, customizations: vscode: extensions: [ ms-vscode.cpptools, ms-vscode.cmake-tools ] 这样我们每次进入容器，VS Code 都会确保这些扩展可用。 3.6.2 指定容器内使用哪个用户（减少权限问题）如果我们的 Dockerfile 里创建了普通用户（比如 vscode），可以在 devcontainer.json 里明确指定： remoteUser: vscode 这样 VS Code 连接后默认就以该用户工作，避免一些文件权限的麻烦。 3.7 常见问题1）进入容器后看不到项目文件通常是挂载位置与 workspaceFolder 不一致。 我们统一约定 /workspace，那么： Compose 要挂载到 /workspace workspaceFolder 也应该是 /workspace 2）VS Code 连接很慢 卡住第一次进入会 build 镜像 + 拉基础镜像，这本身就可能比较慢。如果长时间没有进展，通常与网络拉取镜像、apt 下载速度有关，可以参考后面的内容。 3）容器里生成的文件权限不对多见于容器里用 root 写文件，而本地用户权限不同。解决思路是：在 Dockerfile 里创建普通用户，并在 devcontainer.json 用 remoteUser 指定它。 到这里为止，我们已经完成了开发容器的完整闭环： Docker 能跑 容器能启动、能进入 本地代码与容器同步 VS Code 能直接连接到容器，把编辑终端调试统一到同一个环境里 后续如果我们想把这套环境“做得更舒服”，无非就是继续在 Dockerfile Compose devcontainer.json 里逐步补充：更多依赖、更合理的挂载、更完善的默认配置（例如可能需要配置网络），这有赖于学会更多的docker语法，或者仰仗于 AI ，不过我们最好能做到看懂常用的 Dockerfile 语法。 4. 在终端进入容器与挂载同步：把开发容器“用顺手”前三章我们已经把开发容器跑通了：Docker 能用、容器能启动、VS Code 也能连接。 但在实际开发里，我们仍然会频繁遇到两类问题： 我们到底该用哪些命令进入容器、执行操作？不同命令有什么区别？ 我们的代码、配置、缓存、密钥应该怎么挂载，才能既方便又不容易踩坑？ 这一章就专门解决这两件事：终端操作与挂载同步。把这章吃透以后，开发容器基本就能“长期使用”而不是“能跑就行”。 4.1 三种进入容器的方式为什么进入容器有好几种写法？其实它们的核心差异只有一个：是进入已经在跑的容器，还是临时起一个新容器。 4.1.1 方式 A：docker compose exec（首选）当容器已经在跑时，我们用： docker compose exec dev bash 它的特点是： 进入的是同一个正在运行的容器 我们可以反复进入、退出，不影响容器本身 非常适合容器常驻 + 随时进去开发 如果我们前面用的是 command: sleep infinity，那么这个方式基本就是日常操作的主力。 4.1.2 方式 B：docker exec -it（通用）如果我们知道容器名（例如 container_name: dev），也可以用： docker exec -it dev bash 它与 compose exec 的本质一样：进入已经在跑的容器。区别是它不需要 Compose 上下文，适合我们只记得容器名、或者机器上不方便用 Compose 的场景。 4.1.3 方式 C：docker compose run --rm（一次性任务）如果我们不想容器常驻，或者只想跑一次命令，可以用： docker compose run --rm dev bash 它的特点是： 会临时起一个新的容器实例 退出后容器会被删除（--rm） 适合跑一次性脚本、临时构建、验证某个命令 显然：如果我们想进入“已经在跑的那个容器”，就不要用 run，要用 exec。 4.2 命令速查 构建镜像：docker compose build 启动容器：docker compose up -d 进入容器：docker compose exec dev bash 看容器状态：docker compose ps 看日志：docker compose logs -f 停止并清理：docker compose down 如果我们看到**“容器怎么不见了”“为什么 exec 失败”**，先 docker compose ps 往往就能看出问题。 4.3 挂载容器本质上是一套独立环境，它有自己的文件系统。如果我们什么都不挂载，那么容器里编辑生成的文件，退出容器后很可能就丢了（或者至少不在我们本地项目目录里），这对开发非常不友好。所以开发容器几乎一定会做一件事： 把本地项目目录挂载到容器的 /workspace，让容器直接在我们的项目目录里工作。 这就是我们上文写的这一行： volumes: - ./:/workspace 这样做的直接效果是： 我们在本地改代码，容器里立刻能看到 我们在容器里编译生成文件，本地也能同步出现 Git 仓库仍然是本地仓库，我们的操作习惯不需要改变 4.4 验证实验1）在本地项目目录新建文件 host.txt，随便写点内容 2）进入容器，在 /workspace 下执行： lscat host.txt 能看到文件并读到内容，说明“本地 → 容器”是通的。 3）在容器里再写一个文件： echo hello from container container.txt 4）回到本地项目目录，看看是否出现 container.txt。 能看到文件，说明“容器 → 本地”也是通的。 这一步做完，挂载的概念就不再抽象了。 4.5 配置缓存的挂载为了让开发体验更像本机开发，我们经常会挂载一些个人配置或缓存。 4.5.1 挂载 SSH 与 Git 配置典型写法（Compose）： volumes: - ./:/workspace - ~/.ssh:/home/vscode/.ssh:ro - ~/.gitconfig:/home/vscode/.gitconfig:ro 这里我们特意加了 :ro（只读），因为： .ssh 里通常有私钥，不建议让容器随意改动 .gitconfig 也属于个人配置，读就够了 注意：挂载私钥属于方便但敏感的做法，只建议对可信项目、可信镜像使用。 如果我们不想把整个 .ssh 给容器看，也可以只挂载某个项目专用 key（更安全），但写法会更复杂。 4.5.2 挂载缓存目录很多工具会把缓存放在用户目录里（例如 CMake、包管理器等）。如果我们每次删除容器缓存就没了，体验很差，所以我们可以把缓存目录单独挂载成一个命名卷： services: dev: volumes: - ./:/workspace - dev-cache:/home/vscode/.cachevolumes: dev-cache: 这样即使我们 docker compose down 再 up，缓存也还在，构建速度会更稳定。 4.5.3 挂载数据目录例如我们希望容器生成的输出、日志、测试数据等放在一个明确目录，也可以单独挂载： volumes: - ./data:/data 这种方式的好处是：数据目录和代码目录分开，后续清理、备份更清晰。 4.6 权限问题这是最常遇到、也最容易卡住的问题之一。典型现象是： 容器里 ls 能看到文件 但编辑或生成文件时报 “Permission denied” 或者容器里生成的文件，本地显示“所有者不对” 原因通常是：容器内用户与本地用户不一致，尤其当容器用 root 写文件时更明显。 我们前面在 Dockerfile 里创建普通用户，并尽量对齐 UIDGID，就是为了减少这种问题。如果我们用了 vscode 用户，那么也要记得在 VS Code 的 devcontainer.json 里加： remoteUser: vscode 这样从“构建镜像 → 启动容器 → VS Code 连接”整个链路都会以普通用户工作，权限问题会少很多。 4.7 小结到这里结束，我们已经把两件最关键的事讲清楚并跑通了： 我们知道如何用终端进入容器、什么时候用 exec、什么时候用 run 我们知道挂载如何让代码与本地同步，并且能扩展到 SSH、Git 配置与缓存 后面如果我们要继续完善开发容器，重点往往就集中在两处： Dockerfile：把工具链和依赖做得更贴合项目 挂载与权限：把体验做得更顺手、更安全、更稳定","tags":["docker","vscode","devcontainer"]},{"title":"C++17 并发编程：从数据竞争到三大经典同步问题","path":"/blog/2026/02/02/C++17 并发编程：从数据竞争到三大经典同步问题/","content":"1. 并发到底解决什么：并发 vs 并行、线程模型与现实约束1.1 并发（Concurrency）vs 并行（Parallelism） Concurrency is when two tasks can start, run, and complete in overlapping time periods. Parallelism is when tasks literally run at the same time, eg. on a multi-core processor. Concurrency is the composition of independently executing processes, while parallelism is the simultaneous execution of (possibly related) computations. Concurrency is about dealing with lots of things at once. Parallelism is about doing lots of things at once. An application can be concurrent – but not parallel, which means that it processes more than one task at the same time, but no two tasks are executing at same time instant. An application can be parallel – but not concurrent, which means that it processes multiple sub-tasks of a task in multi-core CPU at same time. An application can be neither parallel – nor concurrent, which means that it processes all tasks one at a time, sequentially. An application can be both parallel – and concurrent, which means that it processes multiple tasks concurrently in multi-core CPU at same time. Vipin Jain. Differences between concurrency vs. parallelism 并发可以理解为一种“同时”的假象：操作系统把 CPU 的时间切成很多很短的时间片，在多个任务之间快速切换。由于切换速度足够快，人就会感觉好像所有任务都在同时运行。哪怕你的电脑只有单核 CPU，依然可以一边听歌、一边下载文件、一边开着编辑器敲代码——任务在宏观上同时进行，微观上交替推进。 而并行则是“真正的同时”：它依赖多个计算单元（多核 CPU、多个处理器，甚至 GPU 的大量计算核心），让多个任务在同一时刻各占一个核心执行。当你有 4 核 CPU 时，理论上可以让 4 段计算密集型任务在同一时间段内各跑各的，这才是并行的典型场景：任务在宏观和微观上都同时推进。 并发更多提升的是系统的响应性与吞吐能力（例如：程序不会卡死在某个慢操作上，而是能继续处理别的请求）；并行才更直接地对应“计算更快”（例如：把一个大计算拆成多块，让多个核心一起算） 1.2 进程 vs 线程：为什么同步问题通常在“线程共享内存”下最典型 在操作系统视角里，**进程（process）和线程（thread）**都可以理解为“正在运行的程序实体”，但它们的核心区别在于：资源隔离的边界在哪里。 进程是操作系统资源分配的最小单位,一个进程可以包含多个线程。每个进程都拥有独立的内存空间,系统再进行资源分配时会为进程分配独立的内存区域。而线程是执行程序的最小单位,多个线程共享同一个进程的内存空间。 1) 进程进程通常被当作操作系统进行资源管理与隔离的基本单位。你可以把一个进程理解为： 一套独立的虚拟地址空间（进程认为自己拥有整个操作系统的全部资源） 一组独立的资源：打开的文件描述符表、信号处理方式、当前工作目录、权限信息等 在 Linux 里，进程的核心管理结构通常被抽象为 PCB（Process Control Block，进程控制块）。在实现层面，Linux 里对应的是 task_struct 之类的内核数据结构，用来记录这个执行实体的状态：调度信息、寄存器上下文、信号、内存映射、文件表指针等。 Process state: Stores whether the process is running, waiting, ready, or terminated. Process number Or PID: Every process is assigned a unique id known as process ID or PID. Program counter:Program Counter stores the address of the next instruction that is to be executed for the process. Register:Registers in the PCB, it is a data structure. When a processes is running and it’s time slice expires, the current value of process specific registers would be stored in the PCB and the process would be swapped out. When the process is scheduled to be run, the register values is read from the PCB and written to the CPU registers. This is the main purpose of the registers in the PCB. Memory limits: This field contains the information about memory management system used by the operating system. This may include page tables, segment tables, etc. List of Open files: This information includes the list of files opened for a process. 不需要记住所有字段，只要抓住一点：PCB task_struct 是内核用来“记住一个执行实体是谁、跑到哪、拥有什么资源”的地方。 2) 线程线程更像是进程内部的“执行流”。同一个进程内的多个线程，通常共享： 同一份虚拟地址空间（也就是同一份代码全局变量堆内存等） 同一组进程级资源（例如文件描述符表等） 但每个线程也有自己的私有部分，最典型的是： 线程栈（stack）：函数调用链、本地变量（自动变量）通常都在这里 寄存器上下文：包括程序计数器、栈指针等（用于上下文切换） 线程本地存储（TLS）等 Linux 里“线程”和“进程”在内核层面其实非常接近：它们都可以用同一种内核对象（仍然是 task_struct）来表示。差别更多体现在：不同的 task_struct 是否共享同一份资源指针（比如地址空间、文件表）。也就是说，线程本质上是“共享资源更多的任务”。 3) 程序的虚拟内存布局：为什么线程共享内存会触发同步问题要解释为什么线程共享内存会触发同步问题，关键是搞清楚：线程到底共享了哪些内存区域。 一个典型进程的虚拟地址空间可以粗略分成这些部分： 代码段（text segment）：程序机器指令，通常是只读（共享也无所谓，因为不改） 只读数据段（rodata）：字符串常量、只读全局常量等 已初始化数据段（data segment）：带初值的全局静态变量 未初始化数据段（BSS segment）：未显式初始化的全局静态变量（启动时会被置 0） 堆（heap）：动态内存分配区域（new/malloc），通常由所有线程共享 栈（stack）：每个线程各自独立的一块栈空间（局部变量一般线程私”，前提是你没把它的地址共享出去） 结论非常直接： 同一进程的多个线程，会共享 dataBSSheap 等区域。 这些区域里一旦有“可变共享数据”，就必须考虑并发访问的正确性，这就是同步问题最典型的来源。 对比一下进程：不同进程的地址空间天然隔离，一个进程里 counter++ 并不会直接把另一个进程的 counter 搞乱。而线程是“默认共享内存”，所以同步问题在多线程下几乎是“必考题”。 1.3 线程越多越好吗？线程数和性能之间通常不是线性关系，线程数增加到某个点以后，吞吐开始下降、延迟开始上升，出现一个“先好后坏”的拐点。 线程并不是“越多越快”的万能解。即使暂时不考虑锁竞争与共享数据争用，仅从系统成本出发，线程数增长也会很快进入收益递减，甚至出现吞吐下降、延迟上升的拐点。主要原因可以拆成三类：创建开销、上下文切换开销、资源占用。 1) 创建开销：线程不是零成本对象创建线程意味着操作系统需要为新的执行流建立完整的运行环境，典型工作包括： 分配并初始化线程栈及其保护页（guard page） 初始化线程上下文（寄存器初始状态、线程入口等） 在内核侧创建注册对应的调度实体，使其能够被调度器管理（例如加入就绪队列、设置调度参数等） 这些步骤决定了线程创建并非“轻量级操作”。当系统以“任务来了就新建线程”的方式扩展并发度时，创建成本会直接体现在启动延迟与系统抖动上，尤其是在短任务场景中，创建销毁线程的开销甚至可能大于任务本身。 启动延迟 就是：任务来了以后，并不会立刻开始做正事，而是要先把线程“建起来”。创建线程需要准备运行环境（栈、上下文、注册进调度器），这些步骤本身就要时间，所以你会看到“任务开始执行”被推迟。 系统抖动 就是：即使同样的任务、同样的机器，延迟也会忽快忽慢。线程一多，调度与切换更频繁，谁先拿到时间片、缓存是否还热都变得更不稳定，于是耗时波动变大，尾延迟更明显。 2) 上下文切换开销：CPU 时间被消耗在“切换”而非“计算”当线程数明显超过 CPU 核心数时，多个线程只能依赖时间片轮转共享计算资源。调度的直接结果就是更频繁的上下文切换，其成本主要来自： 保存恢复寄存器状态、栈指针、程序计数器等执行上下文 缓存局部性被破坏：切换到另一个线程后，原线程的热数据可能已不在 L1L2 缓存中 TLB 及相关地址转换缓存的有效性下降（尤其在工作集较大、线程频繁切换时更明显） 因此你会观察到一个典型现象：线程继续增加时，CPU 使用率可能保持很高，但“有效计算”占比下降，系统时间更多被消耗在调度与恢复现场上，最终体现为吞吐下降、尾延迟变差。 3) 资源占用：线程数量首先消耗的是内存与内核管理开销每个线程都至少需要一块独立的线程栈和相应的内核管理开销。线程数一多，内存压力会上来，系统也要维护更多调度对象。结果就是：线程开到一定规模后，不是性能更好，而是更容易出现资源紧张、创建失败或整体变慢。 4) 结论：线程数不是越多越好，而是要“够用且可控”所以，线程数也存在一个非常现实的上限：系统内存与调度能力。 线程数的上限，往往先被资源（尤其是线程栈内存与调度开销）卡住，而不是被“任务是否够多”卡住。 CPU 密集型任务：线程数通常接近 CPU 核心数（或略多一点点）更合理。std::thread::hardware_concurrency() 可以作为一个粗略参考值。 IO 密集型任务：确实可以开得比核心数多，但仍然会遇到“线程栈占用 + 调度切换”导致的收益递减。 工程上更常见的做法是线程池：让线程数保持稳定可控，避免“每来一个任务就创建一个线程”的资源失控问题。后面讲生产者-消费者时，你会发现它本质上就在解决同一个主题：让任务排队，而不是让线程无限增长。 2. 并发为什么会错：并发代码之所以“容易写错”，本质原因并不神秘：多条执行流的指令会以各种不可预测的顺序交错（interleaving）。一旦这些交错发生在“共享数据的读写”上，就可能导致：同样的输入、同样的代码，每次运行输出都不一样，甚至在 C++ 里直接进入未定义行为。这种“输出随调度时序变化”的现象被称为 indeterminate（不确定性），这种不确定性往往来自于未被保护的共享资源访问。 2.1 Race Condition vs Data Race先把两个经常被混用的词分清楚。 Race Condition（竞态条件）：这是操作系统并发编程里更“泛化”的概念。它强调的是：结果依赖执行时序。当多个线程“差不多同时”进入某段访问共享资源的代码（临界区），程序结果就可能变得不可预测。OSTEP 在并发导论里就用“race condition（或更具体地说 data race）”来描述这种时序依赖导致的不确定结果。 注意：在系统教材里，race condition data race 有时会被放在同一语境里讲；但在 C++ 语言层面，data race有非常严格的定义。 Data Race（数据竞争，C++ 语义）：在 C++11 及之后的内存模型里，data race 是一个更“硬”的术语：当两个线程对同一内存位置发生冲突访问（至少一个是写），且它们之间没有建立 happens-before 关系（例如通过互斥锁同步），程序行为就是未定义（Undefined Behavior, UB）。cppreference 明确写到：一旦发生 data race，程序行为未定义，并给出了类似 cnt++ 的例子。 容易混淆的点（Tip）“跑起来没错”并不能证明没有 data race。因为 UB 的可怕之处在于：编译器和运行时不再需要对你的程序做任何保证，某次优化、某个平台、某个负载下才爆，是常见现象。 2.2 原子性（Atomicity）与临界区（Critical Section）并发错误经常出现在一种场景里：一段逻辑看起来像“一个操作”，实际上是多个步骤。例如 counter++，你在语义上想表达“加一”，但在机器层面通常会拆成三步：读（load）→ 改（add）→ 写（store）。 这类“必须整体不可分割”的代码片段，就是我们在并发里反复提到的 临界区（critical section）：它访问共享变量或共享资源，不能被多个线程同时执行。OSTEP 给出的定义非常直接：临界区是一段访问共享资源的代码，必须确保同一时间最多一个线程进入。 进一步地，我们真正想要的是让临界区“看起来像一条原子指令一样执行”（也就是把一串指令当作不可被并发打断的整体）。加锁的目标就是让临界区“仿佛作为一条单独的原子指令执行”。 2.3 示例：多线程下的自增操作为什么线程不安全现在用最经典的例子把“交错执行”带来的问题落地：多个线程同时对一个共享计数器做 ++。 #include iostream#include threadint a = 0; // 共享变量void add_one_million() for (int i = 0; i 1000000; ++i) ++a; // data race: 未同步的并发写 int main() std::thread t1(add_one_million); std::thread t2(add_one_million); t1.join(); t2.join(); std::cout expected: 2000000 ; std::cout actual: a ; return 0; 直觉上，如果 2 个线程各自执行 100 万次 ++，结果应该是 200 万。但你把这段代码放到多线程里跑，常常会发现：最终结果小于预期，而且每次运行还可能不一样。 关键原因在于：++** 看起来是“一步”，其实通常可以拆成三步完成**： 读出来：把变量当前的值从内存读到 CPU 的寄存器里 加一：在寄存器里把这个值 +1 写回去：把加完后的结果再写回内存中的变量 单线程时，这三步总是按顺序完成，没问题；但在多线程时，两个线程可能会在这三步中交错执行。一旦发生“交错”，就可能出现这种情况： 线程 1 先读到 a = 100（还没来得及写回） 线程 2 也读到 a = 100 线程 1 加一写回 a = 101 线程 2 也加一写回 a = 101 你看，两个线程都做了“加一”，但结果只从 100 变成了 101，本该变成 102 的那一次增量就丢了。这就是为什么多线程下 ++ 会出现“算少了”的现象。 更关键的是：在 C++ 的语义里，这种写法不仅“可能算错”，而是直接构成 data race，行为未定义。cppreference 的多线程与数据竞争章节甚至把这个例子作为典型反例，并明确标注为 undefined behavior；同时也展示了把 int 换成 std::atomicint 后就变为良定义行为。 这不是概率问题。并发下指令交错是常态，你只能控制交错发生时是否仍然正确，而不能指望它别交错。这种现象被称为 indeterminate：输出随线程运行时序变化而变化。 2.4 从“线程安全”自然过渡到“可重入”（避免引入突兀）到这里，你基本已经能形成一个清晰的判断标准：所谓线程安全，核心就是“并发调用时仍然正确”，通常意味着你要么避免共享可变状态，要么用同步手段建立正确的执行关系。 而**可重入（re-entrant）**经常被拿来和线程安全一起出现，但它关注的是另一个维度：某个操作在“尚未完成时”又被再次进入，是否仍然正确。 比如：同一个函数正在执行过程中（可能因为中断、信号处理、回调、递归等原因）再次进入这个函数，如果它内部依赖某些隐藏的可变共享状态（静态缓冲区、全局临时变量等），就可能出问题。 二者的区分很清晰：线程安全是“多线程同时调用也安全”；可重入是“在一次调用尚未完成时再次调用也安全”，因此可重入通常是更强的要求。另外，POSIX 的白皮书也给出了“reentrant function”的规范性表述（强调多线程交错调用时效果应等价于某种串行顺序）。 2.5 活性问题：死锁活锁饥饿并发还有一类更隐蔽的风险：程序不再前进（lack of progress）。典型包括： Deadlock（死锁）：所有线程都卡住了，互相等待资源，谁也无法继续。 CS537 的讲义用一句话概括：一种策略让所有线程都“stuck”，无人能前进。 另一个讲义进一步强调了死锁的本质：每个实体都在等待别人持有的资源，形成等待环。 死锁的发生必须同时满足四个必要条件：互斥条件、请求与保持条件、不剥夺条件和循环等待条件。 Starvation（饥饿）：不是所有线程都卡住，而是某个线程可能无限期等不到运行或资源。 CS537 讲义把它描述为：策略导致某些线程在某些情况下长期不执行。 Livelock（活锁）：所有线程都在“忙”，但一直在做无效动作，仍然没有进展。 CS537 讲义的表述非常直观：大家一直做事，但永远无法取得进展。 3. 线程互斥（Mutual Exclusion）3.1 原子操作（Atomic）：什么时候“无需锁”，什么时候“必须锁”在上一章你已经看到：对普通 int 做并发 ++，不仅“可能算错”，在 C++ 语义里更是 data race → 未定义行为。这一章我们把视角切到 std::atomic：它能把哪些问题变成“良定义”，又在哪些地方绝对不该用 atomic 代替锁。 3.1.1 std::atomic 的基本语义（C++17）std::atomicT 的核心承诺很简单：对同一个原子对象的并发访问，语言层面给出清晰语义。你最常用的是两类操作： 原子读写：load() store() 原子读-改-写（RMW, read-modify-write）：把“读旧值 + 计算新值 + 写回”合成一个不可分割的整体，例如 fetch_add（以及常见的 ++counter）。在文档里，fetch_add 被明确标注为 read-modify-write 操作。 这就是为什么把共享变量从 int 换成 std::atomicint 后，多线程下的累加能稳定得到正确结果：你不再是在并发环境里“拆成三步地更新”，而是在做语言定义的原子 RMW。 #include iostream#include thread#include atomicstd::atomicint a0;void add_one_million() for (int i = 0; i 1000000; ++i) ++a; // data race: 未同步的并发写 int main() std::thread t1(add_one_million); std::thread t2(add_one_million); t1.join(); t2.join(); std::cout expected: 2000000 ; std::cout actual: a ; return 0; atomic 解决的是“单变量原子性”，不是“任意逻辑一致性”把一个变量做成 atomic，只能保证这个变量的每次读写更新是良定义的；如果你的正确性依赖“多个共享状态一起保持某种关系”，那就已经超出 atomic 的能力边界。 在某些设置下（比如开启优化、跑在多核 CPU 上），编译器和 CPU 可能会为了性能，把原子操作及其周围的普通读写，在“对外可见的顺序”上做调整。你写代码时看到的是“先 A 再 B”，但另一个线程观察到的效果（例如被编译器优化了后），可能更像“B 先发生、A 后发生”。这类现象通常就用一个词概括：指令重排序。 它会造成非常经典、也最容易理解的并发 bug：“信号先到，数据后到”。例如你在线程 A 里写： 先把数据写好（普通变量） 再把 ready = true（原子变量）当作“通知” 直觉上，线程 B 只要看到 ready == true，就应该能读到最新数据；但如果没有足够的约束，B 可能出现：看到了 readytrue，却还没看到数据更新的情况。这里不是说代码“乱执行”，而是说跨线程观察时，写入被看见的先后顺序可能不符合你的直觉。 为了解决这种“顺序可见性”问题，C++ 给原子操作设计了一个参数：std::memory_order，用来规定原子操作周围的内存访问应该按多强的规则排序。 而你不显式写 memory_order 时，标准库的默认行为是：按顺序一致（memory_order_seq_cst）来处理。你可以把它理解为“最保守、最不容易写错、也最容易推理”的默认规则：让不同线程看到的原子操作顺序尽量一致，从而降低“信号先到、数据后到”这种反直觉现象。 （像 fetch_add 这类原子 RMW 操作也支持指定 order，默认不写就走上述默认规则。） 3.1.2 CAS（Compare-And-SwapExchange）前面我们用 fetch_add 修复了多线程自增问题，这很好理解：“加一”这种规则太常见了，所以标准库直接给了现成接口。但我们很快会遇到下一类需求： 我不想“加一”，我想按自己的规则更新：例如“把值更新成更大的那个”“只有满足某个条件才更新”“基于旧值算出新值再写回”。标准库没给现成接口怎么办？ 3.1.2.1 CAS 在干什么？先从“并发下值会变”说起在并发环境里，你常常会遇到一个事实：你刚读到的值，下一秒可能就被别的线程改了。 所以问题变成：我能不能这样做—— 我先读到一个值 expected 我基于它计算出一个新值 desired 但在写回之前，我希望确认：“它还是不是我刚才看到的那个值？” 如果不是，那说明有人插队修改过，我就不能盲目写回。 CAS 就是用来解决这一点的。它的意思可以用一句话说清楚：**“如果当前值还等于 expected，就把它改成 desired；否则不改，并把实际值告诉我。”**在 C++ 里，这个操作叫 compare_exchange_weak/compare_exchange_strong（compare-and-exchange）。 3.1.2.2 为什么 CAS 总是和“循环”绑定？因为 CAS 的失败并不是异常情况，反而是并发下的常态： 你读到 expected 别的线程抢先把值改了 你 CAS 失败 失败时 expected 会被更新成“当前真实值”，你再基于新值计算、继续尝试 所以 CAS 的标准用法几乎都是：读-算-尝试（失败就重试）。 compare_exchange_weak 允许一种现象：即使当前值等于 expected，它也可能返回失败（spurious failure）。所以 weak 版本通常就是写在循环里重试，而且在一些平台上它可能更高效。我们可以把它当成一条写法规则：weak 默认就要放在循环里；strong 用在你真的希望“相等就尽量成功”的地方。 3.1.2.3 示例：用 CAS 实现 “原子更新最大值”多个线程同时更新一个共享变量 a，希望它最终等于出现过的最大值（a = max(a, v)）。 #include atomic#include thread#include vector#include iostreamvoid atomic_update_max(std::atomicint a, int v) int expected = a.load(); // 先读一个旧值 while (expected v // 只有 v 更大才需要更新 !a.compare_exchange_weak( expected, v)) // 失败时 expected 会被写成“当前真实值” // 什么也不用做，继续循环即可： // 1) expected 已被更新为最新值 // 2) 重新判断 expected v // 3) 再尝试 CAS int main() std::atomicint a0; std::vectorstd::thread ts; for (int i = 0; i 8; ++i) ts.emplace_back([a, i] // 每个线程尝试把 a 更新成一个不同的值 atomic_update_max(a, i * 10 + 7); ); for (auto t : ts) t.join(); std::cout final a = a.load() ; // 期望是最大值：77 这段代码最重要的两点是： CAS 保证“检查 + 更新”是一个整体：只有当 a 仍等于 expected 时才写入新值，否则就失败。 失败不是坏事：失败时 expected 会自动变成最新值，让你能基于最新状态再算一次、再试一次。 3.1.2 atomic 的边界：为什么“复合不变量”仍需要锁std::atomic 很容易被用过头：把 x、y、state、size 统统改成 atomic，然后下意识觉得“那整体就线程安全了”。问题在于，atomic 提供的是单个变量的原子性，而此时我们真正想要的往往是多个变量之间的整体一致性。 先从“并发下最容易发生的事”说起：你的一次更新如果要改两个变量，哪怕每次 store() 都是原子的，这两次写入之间仍然存在一个窗口。在这个窗口里，另一个线程完全可能插进来读数据，于是它就会读到一个“半更新”的中间态。 举个最简单的例子：你希望永远满足 x y，写线程要做的是“同时调整 x 和 y”。但现实是它只能先改一个，再改另一个；读线程只要恰好夹在中间，就能看到临时违反不变量的组合，比如 x 已经变大，而 y 还没来得及跟上。这里并不是 atomic “失效”，而是：你需要的是一次事务式更新（要么都变，要么都不变），atomic 给不了这种跨变量的一致性快照。 更进一步，在多核系统里，如果你对多个变量进行读写而没有足够的同步约束，不同线程观察到“值变化的顺序”甚至可能不一致：一个线程看到 x 先变，另一个线程看到 y 先变。cppreference 对 memory_order 的说明就明确提到：当多个线程同时读写多个变量时，一个线程可能观察到的变化顺序不同于另一个线程写入顺序，甚至不同读线程之间观察到的顺序也可能不同。 这句话的直观含义是：当你在做“复合状态更新”时，如果没有把它变成一个整体的同步单元，读者看到的世界可能是“拼接出来的”。 所以一旦我们的正确性条件是跨多个共享状态的，例如： x y 必须始终成立 一个对象的多个字段必须同时更新才算“进入新状态” map 与旁路索引必须一致 那么单靠 atomic 往往会把你带到“局部都很原子，但整体仍可能乱”的局面：每个字段单独读写都没 data race，可读到的组合却可能不一致。 atomic 也不保证一定“无锁”。cppreference 明确写到：除了 std::atomic_flag 外，其它 atomic 类型允许用 mutex 或其它锁机制实现；是否 lock-free 需要看 is_lock_free()is_always_lock_free 的结果。 3.1.3 性能陷阱：伪共享（False Sharing） 读者提示：理解下面的“伪共享（false sharing）”需要你先知道三件事：1）CPU 缓存以 cache line 为最小管理单位；2）多核通过 缓存一致性协议维护 cache line 的共享与失效；3）多数 CPU 采用 write-back写缓冲，写入需要通过一致性协议传播到其他核心。伪共享的本质就是：两个线程改的是不同变量，但它们落在同一条 cache line 上，导致一致性协议把整条 cache line 当成“共享热点”来回同步，从而白白产生开销。 In computer science, false sharing is a performance-degrading usage pattern that can arise in systems with distributed, coherent caches at the size of the smallest resource block managed by the caching mechanism. When a system participant attempts to periodically access data that is not being altered by another party, but that data shares a cache block with data that is being altered, the caching protocol may force the first participant to reload the whole cache block despite a lack of logical necessity. The caching system is unaware of activity within this block and forces the first participant to bear the caching system overhead required by true shared access of a resource. By far the most common usage of this term is in modern multiprocessor CPU caches, where memory is cached in lines of some small power of two word size (e.g., 64 aligned, contiguous bytes). If two processors operate on independent data in the same memory address region storable in a single line, the cache coherency mechanisms in the system may force the whole line across the bus or interconnect with every data write, forcing memory stalls in addition to wasting system bandwidth. In some cases, the elimination of false sharing can result in order-of-magnitude performance improvements.False sharing is an inherent artifact of automatically synchronized cache protocols and can also exist in environments such as distributed file systems or databases, but current prevalence is limited to RAM caches. 另一个常见误解是：把锁换成 atomic，就一定更快。并发下经常出现相反情况：atomic 写得越频繁，越可能越慢。 原因往往不在“原子指令本身”，而在缓存一致性：当多个线程频繁写同一个 cache line 上的数据时，该 cache line 会在不同 CPU 核之间反复失效同步，吞吐下降、延迟抖动。这种“两个线程写的是不同变量，但它们恰好落在同一条 cache line 上，于是互相拖累”的现象，就叫 伪共享（false sharing）。 3.2 std::mutex 与为什么不要手写 lock()/unlock() 在上一 节我们已经见过“共享变量并发写会出事”。接下来我们需要一个更工程化的工具：把“临界区”围起来，让同一时刻只有一个线程能进入。C++ 标准库提供的核心原语是 std::mutex，以及一组围绕 RAII 设计的锁管理器（lock_guard/unique_lock/scoped_lock/...）。 std::mutex 的职责很单纯：保护共享数据，避免被多个线程同时访问。问题在于：如果我们手写 lock() unlock()，一旦代码中途 return、throw、或未来某次维护时忘了写 unlock()，就会把互斥量“永远锁住”，其他线程会一直等下去。 下面这段对比代码展示了这个风险：反例里我们故意 throw，然后用 try_lock() 观察互斥量是否还被锁着（避免程序真的卡死）；正例用 RAII 的 std::lock_guard，即使抛异常也会在析构时自动释放。lock_guard 的语义是“构造时加锁、离开作用域析构时解锁”。 #include iostream#include mutex#include stdexceptstd::mutex g_m;void bad() g_m.lock(); throw std::runtime_error(oops); // 永远到不了 unlock g_m.unlock();void good() std::lock_guardstd::mutex lk(g_m); // RAII：作用域结束必解锁 throw std::runtime_error(oops);int main() // 反例：手写 lock/unlock，异常后锁被“遗留” try bad(); catch (...) if (g_m.try_lock()) std::cout [bad] mutex is NOT locked (unexpected) ; g_m.unlock(); else std::cout [bad] mutex is still locked - other threads would block ; // 为了不影响后续演示，这里把它解开 g_m.unlock(); // 正例：lock_guard，异常后锁仍会被释放 try good(); catch (...) if (g_m.try_lock()) std::cout [good] mutex is unlocked (expected) ; g_m.unlock(); else std::cout [good] mutex is still locked (unexpected) ; 我们可以把锁理解为“锁住共享数据的访问权”，而不是“锁住某段代码”。工程上更推荐把 mutex 和它保护的数据一起封装进类，减少“忘记加锁锁错对象”的概率。 3.3 std::lock_guard：默认首选std::lock_guard 是最轻量的 RAII 锁管理器：构造时尝试获取互斥量所有权，离开作用域就释放。它不可拷贝，也不提供中途解锁等复杂能力，所以简单、稳定、开销低。 #include iostream // std::cout#include thread // std::thread#include mutex // std::mutex, std::lock_guard#include stdexcept // std::logic_errorstd::mutex mtx;void print_even (int x) if (x%2==0) std::cout x is even ; else throw (std::logic_error(not even));void print_thread_id (int id) try // using a local lock_guard to lock mtx guarantees unlocking on destruction / exception: std::lock_guardstd::mutex lck (mtx); print_even(id); catch (std::logic_error) std::cout [exception caught] ; int main () std::thread threads[10]; // spawn 10 threads: for (int i=0; i10; ++i) threads[i] = std::thread(print_thread_id,i+1); for (auto th : threads) th.join(); return 0; 3.4 std::unique_lock：为条件变量与更复杂流程准备std::unique_lock 的定位是“更通用的锁所有权管理器”：它支持延迟加锁、try_lock、中途 unlock 重新 lock，还可以移动（transfer ownership）。 下面这段代码展示两个常用模式：延迟加锁（defer_lock）和缩短持锁时间（中途 unlock 把重活放到锁外做）。 #include mutexstd::mutex m;void do_work() std::unique_lockstd::mutex lk(m, std::defer_lock); // 先不锁 // ...做一些不需要锁的准备工作 lk.lock(); // 真正进入临界区 // 修改共享数据（需要锁保护） lk.unlock(); // 尽早放锁 // 重活放在锁外：IO、计算、回调等 3.5 多把锁的标准解：std::scoped_lock std::lock多把锁最常见的风险是死锁：线程 A 先锁 m1 再锁 m2，线程 B 反过来先锁 m2 再锁 m1，两边互相等待就卡住。C++17 推荐的写法是：要么统一顺序，要么一次性锁多把。 std::scoped_lock 是 C++17 引入的 RAII 包装器，支持“持有 0 或多把互斥量”。std::lock 是更底层的“一次性锁多把”的函数，cppreference 也明确提示：scoped_lock 是它的 RAII 包装器，一般更推荐。 #include mutexstruct Account std::mutex m; int balance0;;void transfer(Account from, Account to, int amount) std::scoped_lock lk(from.m, to.m); // C++17：一次性锁多把 from.balance -= amount; to.balance += amount; #include mutexvoid transfer2(Account from, Account to, int amount) std::unique_lockstd::mutex l1(from.m, std::defer_lock); std::unique_lockstd::mutex l2(to.m, std::defer_lock); std::lock(l1, l2); // 一次性获取两把锁 from.balance -= amount; to.balance += amount; 多锁场景我们只保留两条纪律：统一顺序，或使用 scoped_lock/std::lock。不要随手写“先锁 A 再锁 B”的不一致逻辑。 3.6 读多写少：std::shared_mutex + std::shared_lock当共享数据“读远多于写”时，普通 mutex 会把读也串行化，浪费并发性。C++17 提供 std::shared_mutex：它支持两种模式： 共享（shared）：多个线程可同时持有（读锁） 独占（exclusive）：同一时刻只能一个线程持有（写锁） std::shared_lock 用于管理共享锁；而独占锁仍然用 std::unique_lockshared_mutex。 #include shared_mutex#include unordered_mapclass ThreadSafeMap public:int get(int k) const std::shared_lockstd::shared_mutex lk(mu_); // 读：共享锁 auto it = m_.find(k); return it == m_.end() ? 0 : it-second;void set(int k, int v) std::unique_lockstd::shared_mutex lk(mu_); // 写：独占锁 m_[k] = v;private:mutable std::shared_mutex mu_;std::unordered_mapint, int m_;; 配图建议（读多写少示意图）画三个读者同时持有 shared_lock（绿色），一个写者请求独占锁（红色），并标注“写者需等待读者全部释放”。shared_mutex::lock() 的说明里也强调：当已有共享锁独占锁存在时，独占加锁会阻塞。 公平性与“写者是否可能饥饿”依实现与策略而定，不要默认“写一定很快拿到锁”。 4. 线程同步（Synchronization）4.1 条件变量：从忙等到阻塞等待4.1.1 为什么需要 std::condition_variable如果我们在队列空的时候用 while (empty) {} 这种方式反复检查，就属于忙等：CPU 会空转而被白白消耗，而且线程越多锁越热。条件变量的目的就是把这种“反复检查”变成：条件不成立就睡眠阻塞；条件成立被唤醒后再检查一次。 std::condition_variable 的定位是：配合 std::mutex，让线程在某个条件满足之前阻塞等待，并由其他线程在修改条件后发出通知。 4.1.2 wait(lock, predicate) 的标准写法与虚假唤醒条件变量的 wait 返回，只能说明“现在应该再检查一次条件”，而不能说明“条件已经满足”。原因是：标准库允许线程在没有收到 notify_one()/notify_all() 的情况下，从 wait 中返回——这就叫虚假唤醒（spurious wakeup）。cppreference 在描述 std::condition_variable 的等待行为时也明确提到：wait 会挂起线程，直到被通知、超时，或发生虚假唤醒，然后再重新获取互斥锁返回。 为什么要允许这种行为？我们不需要钻到内核细节里，只要记住它是“实现层面为了兼容性与效率而允许的行为”。因此它不是程序逻辑错误，也不是异常能捕获的情况；我们要做的是写出天然兼容虚假唤醒的等待代码。 4.1.2.1 规范写法：醒来以后必须重新检查条件最常见的坑是把等待写成 “if + wait”： std::unique_lockstd::mutex lk(m);if (q.empty()) // 只检查一次 cv.wait(lk); // 可能因为虚假唤醒返回// 如果这里仍然是空队列，下面就出错了auto x = q.front();q.pop(); 这段代码的问题在于：我们只在进入 wait 前检查了一次 q.empty()。如果线程“醒来”时队列仍为空（可能是虚假唤醒，也可能是别的线程先把数据拿走了），我们仍会继续执行 front()/pop()，逻辑就崩了。 正确写法有两种，本质完全一样： 1）推荐写法：用带谓词的 wait 重载它把“醒来后再次检查条件”封装好了： std::unique_lockstd::mutex lk(m);cv.wait(lk, [] return !q.empty(); ); // 条件不满足就继续等auto x = q.front();q.pop(); 这个重载就是用来忽略处理虚假唤醒的。 2）等价写法：手写 while 循环更直白： std::unique_lockstd::mutex lk(m);while (q.empty()) cv.wait(lk);auto x = q.front();q.pop(); 这两种写法的核心思想是一样的：把 wait 当成“可能会醒”的睡眠，而把 predicatewhile 当成“真正的条件门槛”。醒来只是提示我们“再看一眼共享状态”，而不是承诺“状态已经满足”。 等待条件变量时，**默认使用 **cv.wait(lock, predicate)（最不容易写错）。 如果必须用不带 predicate 的 wait，也必须写成 while 重新检查条件，**禁止 **if + wait。 这样写不仅能规避虚假唤醒，也能顺带规避另一类常见问题：通知与条件检查之间的竞态（醒来时条件未必仍成立），整体更稳。 4.2 信号量（Semaphore）4.2.1 信号量是什么：计数许可如果说 mutex 是“同一时刻只能一个人进”，信号量更像“发放若干张许可证”：许可证 0 时允许进入并扣减一张；许可证为 0 时就阻塞等待。这个抽象通常称为计数信号量（counting semaphore）。值只在 01 的情形称为二值信号量（binary semaphore）。 4.2.2 semaphore vs mutexcv：各自擅长什么我们可以用一句话区分三者的“表达力”： mutex：保护临界区一致性（互斥进入） condition_variable：等待“某个条件成立”（需要 mutex + predicate） semaphore：表达“资源数量并发许可”（天然就是计数） 信号量通常不直接保证数据结构一致性：它解决“名额数量”，而共享结构的读写仍可能需要 mutex 来保护。 4.2.3 规范写法标准库的 std::counting_semaphore semaphore 是 C++20 才引入的。在 C++17 里，我们通常做两件事： 正文讲概念与用法 需要代码时，用 mutex + condition_variable + 计数器 模拟一个最小可用的 semaphore（写法与 wait(lock,pred) 一样，自动处理虚假唤醒） 下面给出一个最小实现，以及一个“限制最多 K 个线程同时访问资源”的 demo。 #include condition_variable#include mutexclass Semaphore public:explicit Semaphore(int initial) : count_(initial) void acquire() std::unique_lockstd::mutex lk(m_); cv_.wait(lk, [] return count_ 0; ); --count_;void release() std::lock_guardstd::mutex lk(m_); ++count_; cv_.notify_one();private:std::mutex m_;std::condition_variable cv_;int count_;; #include atomic#include chrono#include iostream#include thread#include vector// 假设 Semaphore 已定义int main() Semaphore sem(3); // 最多 3 个线程同时进入 std::atomicint in_flight0; std::vectorstd::thread ts; for (int i = 0; i 10; ++i) ts.emplace_back([, i] sem.acquire(); int cur = ++in_flight; std::cout enter i , in_flight= cur ; std::this_thread::sleep_for(std::chrono::milliseconds(200)); cur = --in_flight; std::cout leave i , in_flight= cur ; sem.release(); ); for (auto t : ts) t.join(); 如果运行正常，我们会看到 in_flight 的最大值不超过 3。 5. 三个经典同步问题 本章记录“生产者-消费者 读者-写者 哲学家就餐”三个经典问题，目的是用它们把互斥与同步工具串起来，帮助我们建立可复用的并发思维框架。需要提前说明的是：这些问题在不同约束下都有多种解法（例如偏吞吐、偏公平、偏实现简单、偏可验证），本文给出的实现更强调“语义清晰、可读性强、易于推理”，并不保证在所有平台与负载下都是性能最优或策略最公平的版本。读者可以把它们当作参考基线，在理解原理后再按具体业务目标做取舍与优化。 前面我们已经把两类“工具”准备好了： **线程互斥：**用锁把临界区围起来，保证共享数据的一致性 **线程同步：**用条件变量信号量让线程在“条件不满足时阻塞等待”，而不是忙等 这一章我们把它们落到三个经典模型上。重点是抓住“共享状态是什么、什么时候等、什么时候唤醒”。 5.1 生产者-消费者（有界阻塞队列）这个问题对应非常常见的工程场景：一个线程（或多个）产生任务数据，另一个线程（或多个）消费任务数据，中间用一个缓冲区（队列）连接。核心矛盾只有两个： 队列空：消费者不能硬取，需要等待 队列满：生产者不能硬塞，需要等待 这里最容易写错的点，是把等待写成“if + wait”。我们已经强调过：wait 可能因为通知、也可能因为虚假唤醒返回，所以正确写法必须醒来后重新检查条”。 我们先把“队列的共享状态”说清楚：队列的大小 q.size() 决定了两个条件： not_empty：!q.empty()（消费者可以取） not_full：q.size() cap（生产者可以放） 下面是一个 C++17 的解法：一个有界 BlockingQueueT，用一把 mutex 保护队列结构，用两个条件变量分别表达 not_empty 与 not_full（一个常见的用法，值得记住）。 #include condition_variable#include mutex#include queue#include utilitytemplate class Tclass BlockingQueue public:explicit BlockingQueue(size_t cap) : cap_(cap) void push(T v) std::unique_lockstd::mutex lk(m_); cv_not_full_.wait(lk, [] return q_.size() cap_; ); // 等 not_full q_.push(std::move(v)); lk.unlock(); cv_not_empty_.notify_one(); // 状态从 empty - non-empty 的方向推进T pop() std::unique_lockstd::mutex lk(m_); cv_not_empty_.wait(lk, [] return !q_.empty(); ); // 等 not_empty T v = std::move(q_.front()); q_.pop(); lk.unlock(); cv_not_full_.notify_one(); // 状态从 full - non-full 的方向推进 return v;private:const size_t cap_;std::mutex m_;std::condition_variable cv_not_full_;std::condition_variable cv_not_empty_;std::queueT q_;; 5.2 读者-写者（读并发、写独占）我们要解决的场景很典型：同一份共享数据上，读操作彼此并不冲突，所以理想情况下可以并发执行；但写操作会改变共享数据，它必须独占，并且与任何读互斥。换句话说： 允许多个读者同时进入 一旦有写者进入，必须保证没有任何读者写者同时在里面 如果我们直接用普通 std::mutex，当然能做到正确性，但代价是：读也被串行化——明明读之间互不影响，却被迫排队，浪费并发性。C++17 的 std::shared_mutex 就是为此准备的：读者拿共享锁（std::shared_lock），写者拿独占锁（std::unique_lockstd::shared_mutex）。不过，标准库的 shared_mutex 并不承诺“读者优先写者优先严格公平”这类策略；如果我们确实需要可控策略，就要自己实现一把 RWLock（读写锁）。 下面是三种常见策略： 读者优先 RWLock（Readers-preference） 只要没有写者正在写，就尽量放行读者；写者必须等到“所有读者都离开”。优点是读吞吐高，缺点是读源源不断时写者可能等很久。 写者优先 RWLock（Writers-preference） 一旦有写者在排队，就不再放行新的读者；这样写者不会被“不断涌入的读”饿死。优点是写延迟可控，缺点是写多时读延迟会升高。 公平 RWLock（Phase-fair，读写轮转） 让读和写“轮流占用舞台”。当读者批量完成后让一位写者进入，写者完成后再放行一批读者。它不追求严格 FIFO，但能避免某一方长期饥饿，同时实现复杂度可控。 5.3 哲学家就餐（多锁死锁与规避） 哲学家就餐问题可以这样表述，假设有五位哲学家围坐在一张圆形餐桌旁，做以下两件事情之一：吃饭，或者思考。吃东西的时候，他们就停止思考，思考的时候也停止吃东西。餐桌上有五碗意大利面，每位哲学家之间各有一只餐叉。因为用一只餐叉很难吃到意大利面，所以假设哲学家必须用两只餐叉吃东西。他们只能使用自己左右手边的那两只餐叉。哲学家就餐问题有时也用米饭和五根筷子而不是意大利面和餐叉来描述，因为吃米饭必须用两根筷子。 这个问题不考虑意大利面有多少，也不考虑哲学家的胃有多大。假设两者都是无限大。 问题在于如何设计一套规则，使得在哲学家们在完全不交谈，也就是无法知道其他人可能在什么时候要吃饭或者思考的情况下，可以在这两种状态下永远交替下去。 哲学家就餐用来演示一个非常典型的活性风险：死锁。场景是环形资源：每个人需要两把叉才能吃。如果所有人都按同一种顺序拿叉（比如都先拿左叉），就可能出现： 每个人都拿到一把叉 然后都在等待另一把叉 形成环路等待，系统没有任何线程能推进 这一题的价值在于：解决“多把锁怎么拿才不会死锁”的问题。我们的原则很简单： 多锁要么统一顺序 要么一次性拿（交给标准库的死锁规避算法） 可以使用 std::scoped_lock 进行一种标准的实现： #include algorithm#include mutex#include thread#include vectorint main() constexpr int N = 5; std::mutex forks[N]; auto philosopher = [](int i) int left = i; int right = (i + 1) % N; int a = std::min(left, right); int b = std::max(left, right); std::scoped_lock lk(forks[a], forks[b]); // C++17：一次性锁两把（带死锁规避） // eat... ; std::vectorstd::thread ts; for (int i = 0; i N; ++i) ts.emplace_back(philosopher, i); for (auto t : ts) t.join(); 我们把“统一顺序”说得更直白一点：所有线程都遵守“先拿编号小的，再拿编号大的”，就不可能形成环路等待。再加上 scoped_lock 的“一次性上锁”，实现上更稳。 参考资料melonstreet - 博客园 进程控制块PCB结构体 task_struct 描述 - ProLyn - 博客园 Process Control Block in OS - GeeksforGeeks Linux内存管理4—虚拟地址空间管理 - jasonactions - 博客园 Starvation and Livelock - GeeksforGeeks https://en.wikipedia.org/wiki/False_sharing https://en.cppreference.com/w/ https://cplusplus.com/reference/mutex/lock_guard 哲学家进餐问题(Dining philosophers problem) https://zh.wikipedia.org/wiki/%E5%93%B2%E5%AD%A6%E5%AE%B6%E5%B0%B1%E9%A4%90%E9%97%AE%E9%A2%98","tags":["cpp","os"]},{"title":"关于","path":"/blog/about/index.html","content":"关于你好，这里是 Kamil Liu，本科就读于天津大学软件工程，预计 2026 年 6 月毕业，并将于 2026 年 8 月前往新加坡南洋理工大学攻读 Cyber Security。Hi, I’m Kamil Liu. I study Software Engineering at Tianjin University, expect to graduate in Jun 2026, and will start Cyber Security at Nanyang Technological University in Singapore in Aug 2026. 近日心血来潮开始撰写一些技术博客，我的合作伙伴是各种各样的 AI；我会把自己碎片化的专业记忆与互联网资料缝缝补补，如有谬误敬请原谅，所有内容仅供参考，主要目的是个人学习和复习。Recently I started writing tech posts, often with help from various AIs. I stitch together fragmented knowledge with online sources; mistakes are possible, everything is for reference only, and the main goal is personal learning and review. 如您有意见和建议，欢迎通过邮件联系，我将不胜感激！If you have feedback or suggestions, please reach me by email—I would be very grateful! 邮箱：kamill7779@outlook.comEmail: kamill7779@outlook.com"},{"title":"时间线","path":"/blog/timeline/index.html","content":"2026-01收到南洋理工大学offer，欣然规往received offer from Nanyang Technology University and planned to accept2022-08入驻天津大学喵~admitted by Tianjin University"},{"title":"友链","path":"/blog/friends/index.html","content":"欢迎交换友链，可以通过邮件或 GitHub 联系我。 朋友们HexoStellarMDN Web Docs 站点推荐Hexo 官方Hexo 官方网站与文档Stellar 文档Stellar 主题使用说明与示例GitHub代码托管与协作平台"}]