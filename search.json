[{"title":"C++17 并发编程：从数据竞争到三大经典同步问题","path":"/blog/2026/02/02/C++17 并发编程：从数据竞争到三大经典同步问题/","content":"并发到底解决什么：并发 vs 并行、线程模型与现实约束1.1 并发（Concurrency）vs 并行（Parallelism） Concurrency is when two tasks can start, run, and complete in overlapping time periods. Parallelism is when tasks literally run at the same time, eg. on a multi-core processor. Concurrency is the composition of independently executing processes, while parallelism is the simultaneous execution of (possibly related) computations. Concurrency is about dealing with lots of things at once. Parallelism is about doing lots of things at once. An application can be concurrent – but not parallel, which means that it processes more than one task at the same time, but no two tasks are executing at same time instant. An application can be parallel – but not concurrent, which means that it processes multiple sub-tasks of a task in multi-core CPU at same time. An application can be neither parallel – nor concurrent, which means that it processes all tasks one at a time, sequentially. An application can be both parallel – and concurrent, which means that it processes multiple tasks concurrently in multi-core CPU at same time. Vipin Jain. Differences between concurrency vs. parallelism 并发可以理解为一种“同时”的假象：操作系统把 CPU 的时间切成很多很短的时间片，在多个任务之间快速切换。由于切换速度足够快，人就会感觉好像所有任务都在同时运行。哪怕你的电脑只有单核 CPU，依然可以一边听歌、一边下载文件、一边开着编辑器敲代码——任务在宏观上同时进行，微观上交替推进。 而并行则是“真正的同时”：它依赖多个计算单元（多核 CPU、多个处理器，甚至 GPU 的大量计算核心），让多个任务在同一时刻各占一个核心执行。当你有 4 核 CPU 时，理论上可以让 4 段计算密集型任务在同一时间段内各跑各的，这才是并行的典型场景：任务在宏观和微观上都同时推进。 :::info并发更多提升的是系统的响应性与吞吐能力（例如：程序不会卡死在某个慢操作上，而是能继续处理别的请求）；并行才更直接地对应“计算更快”（例如：把一个大计算拆成多块，让多个核心一起算） ::: 1.2 进程 vs 线程：为什么同步问题通常在“线程共享内存”下最典型 在操作系统视角里，**进程（process）和线程（thread）**都可以理解为“正在运行的程序实体”，但它们的核心区别在于：资源隔离的边界在哪里。 进程是操作系统资源分配的最小单位,一个进程可以包含多个线程。每个进程都拥有独立的内存空间,系统再进行资源分配时会为进程分配独立的内存区域。而线程是执行程序的最小单位,多个线程共享同一个进程的内存空间。 1) 进程进程通常被当作操作系统进行资源管理与隔离的基本单位。你可以把一个进程理解为： 一套独立的虚拟地址空间（进程认为自己拥有整个操作系统的全部资源） 一组独立的资源：打开的文件描述符表、信号处理方式、当前工作目录、权限信息等 在 Linux 里，进程的核心管理结构通常被抽象为 PCB（Process Control Block，进程控制块）。在实现层面，Linux 里对应的是 task_struct 之类的内核数据结构，用来记录这个执行实体的状态：调度信息、寄存器上下文、信号、内存映射、文件表指针等。 Process state: Stores whether the process is running, waiting, ready, or terminated. Process number Or PID: Every process is assigned a unique id known as process ID or PID. Program counter:Program Counter stores the address of the next instruction that is to be executed for the process. Register:Registers in the PCB, it is a data structure. When a processes is running and it’s time slice expires, the current value of process specific registers would be stored in the PCB and the process would be swapped out. When the process is scheduled to be run, the register values is read from the PCB and written to the CPU registers. This is the main purpose of the registers in the PCB. Memory limits: This field contains the information about memory management system used by the operating system. This may include page tables, segment tables, etc. List of Open files: This information includes the list of files opened for a process. 不需要记住所有字段，只要抓住一点：PCB**task_struct**** 是内核用来“记住一个执行实体是谁、跑到哪、拥有什么资源”的地方。** 2) 线程线程更像是进程内部的“执行流”。同一个进程内的多个线程，通常共享： 同一份虚拟地址空间（也就是同一份代码全局变量堆内存等） 同一组进程级资源（例如文件描述符表等） 但每个线程也有自己的私有部分，最典型的是： 线程栈（stack）：函数调用链、本地变量（自动变量）通常都在这里 寄存器上下文：包括程序计数器、栈指针等（用于上下文切换） 线程本地存储（TLS）等（可选点到） Linux 里“线程”和“进程”在内核层面其实非常接近：它们都可以用同一种内核对象（仍然是 task_struct）来表示。差别更多体现在：不同的 **task_struct** 是否共享同一份资源指针（比如地址空间、文件表）。也就是说，线程本质上是“共享资源更多的任务”。 3) 程序的虚拟内存布局：为什么线程共享内存会触发同步问题要解释为什么线程共享内存会触发同步问题，关键是搞清楚：线程到底共享了哪些内存区域。 一个典型进程的虚拟地址空间可以粗略分成这些部分： 代码段（text segment）：程序机器指令，通常是只读（共享也无所谓，因为不改） 只读数据段（rodata）：字符串常量、只读全局常量等 已初始化数据段（data segment）：带初值的全局静态变量 未初始化数据段（BSS segment）：未显式初始化的全局静态变量（启动时会被置 0） 堆（heap）：动态内存分配区域（new/malloc），通常由所有线程共享 栈（stack）：每个线程各自独立的一块栈空间（局部变量一般线程私”，前提是你没把它的地址共享出去） 结论非常直接：**同一进程的多个线程，会共享 dataBSSheap 等区域。**这些区域里一旦有“可变共享数据”，就必须考虑并发访问的正确性，这就是同步问题最典型的来源。 对比一下进程：不同进程的地址空间天然隔离，一个进程里 counter++ 并不会直接把另一个进程的 counter 搞乱。而线程是“默认共享内存”，所以同步问题在多线程下几乎是“必考题”。 1.3 线程越多越好吗？线程数和性能之间通常不是线性关系，线程数增加到某个点以后，吞吐开始下降、延迟开始上升，出现一个“先好后坏”的拐点。 线程并不是“越多越快”的万能解。即使暂时不考虑锁竞争与共享数据争用，仅从系统成本出发，线程数增长也会很快进入收益递减，甚至出现吞吐下降、延迟上升的拐点。主要原因可以拆成三类：创建开销、上下文切换开销、资源占用。 1) 创建开销：线程不是零成本对象创建线程意味着操作系统需要为新的执行流建立完整的运行环境，典型工作包括： 分配并初始化线程栈及其保护页（guard page） 初始化线程上下文（寄存器初始状态、线程入口等） 在内核侧创建注册对应的调度实体，使其能够被调度器管理（例如加入就绪队列、设置调度参数等） 这些步骤决定了线程创建并非“轻量级操作”。当系统以“任务来了就新建线程”的方式扩展并发度时，创建成本会直接体现在启动延迟与系统抖动上，尤其是在短任务场景中，创建销毁线程的开销甚至可能大于任务本身。 :::info启动延迟就是：任务来了以后，并不会立刻开始做正事，而是要先把线程“建起来”。创建线程需要准备运行环境（栈、上下文、注册进调度器），这些步骤本身就要时间，所以你会看到“任务开始执行”被推迟。 系统抖动就是：即使同样的任务、同样的机器，延迟也会忽快忽慢。线程一多，调度与切换更频繁，谁先拿到时间片、缓存是否还热都变得更不稳定，于是耗时波动变大，尾延迟更明显。 ::: 2) 上下文切换开销：CPU 时间被消耗在“切换”而非“计算”当线程数明显超过 CPU 核心数时，多个线程只能依赖时间片轮转共享计算资源。调度的直接结果就是更频繁的上下文切换，其成本主要来自： 保存恢复寄存器状态、栈指针、程序计数器等执行上下文 缓存局部性被破坏：切换到另一个线程后，原线程的热数据可能已不在 L1L2 缓存中 TLB 及相关地址转换缓存的有效性下降（尤其在工作集较大、线程频繁切换时更明显） 因此你会观察到一个典型现象：线程继续增加时，CPU 使用率可能保持很高，但“有效计算”占比下降，系统时间更多被消耗在调度与恢复现场上，最终体现为吞吐下降、尾延迟变差。 3) 资源占用：线程数量首先消耗的是内存与内核管理开销每个线程都至少需要一块独立的线程栈和相应的内核管理开销。线程数一多，内存压力会上来，系统也要维护更多调度对象。结果就是：线程开到一定规模后，不是性能更好，而是更容易出现资源紧张、创建失败或整体变慢。 4) 结论：线程数不是越多越好，而是要“够用且可控”所以，线程数也存在一个非常现实的上限：系统内存与调度能力。 线程数的上限，往往先被资源（尤其是线程栈内存与调度开销）卡住，而不是被“任务是否够多”卡住。 :::info CPU 密集型任务：线程数通常接近 CPU 核心数（或略多一点点）更合理。std::thread::hardware_concurrency() 可以作为一个粗略参考值。 IO 密集型任务：确实可以开得比核心数多，但仍然会遇到“线程栈占用 + 调度切换”导致的收益递减。 工程上更常见的做法是线程池：让线程数保持稳定可控，避免“每来一个任务就创建一个线程”的资源失控问题。后面讲生产者-消费者时，你会发现它本质上就在解决同一个主题：让任务排队，而不是让线程无限增长。 ::: 并发为什么会错：竞态条件、数据竞争、原子性与“多线程 +1”直观演示并发代码之所以“容易写错”，本质原因并不神秘：多条执行流的指令会以各种不可预测的顺序交错（interleaving）。一旦这些交错发生在“共享数据的读写”上，就可能导致：同样的输入、同样的代码，每次运行输出都不一样，甚至在 C++ 里直接进入未定义行为。这种“输出随调度时序变化”的现象被称为 indeterminate（不确定性），这种不确定性往往来自于未被保护的共享资源访问。 2.1 术语先立住：Race Condition vs Data Race先把两个经常被混用的词分清楚。 Race Condition（竞态条件）：这是操作系统并发编程里更“泛化”的概念。它强调的是：结果依赖执行时序。当多个线程“差不多同时”进入某段访问共享资源的代码（临界区），程序结果就可能变得不可预测。OSTEP 在并发导论里就用“race condition（或更具体地说 data race）”来描述这种时序依赖导致的不确定结果。 注意：在系统教材里，race condition data race 有时会被放在同一语境里讲；但在 C++ 语言层面，data race有非常严格的定义。 Data Race（数据竞争，C++ 语义）：在 C++11 及之后的内存模型里，data race 是一个更“硬”的术语：当两个线程对同一内存位置发生冲突访问（至少一个是写），且它们之间没有建立 happens-before 关系（例如通过互斥锁同步），程序行为就是未定义（Undefined Behavior, UB）。cppreference 明确写到：一旦发生 data race，程序行为未定义，并给出了类似 cnt++ 的例子。 容易混淆的点（Tip）“跑起来没错”并不能证明没有 data race。因为 UB 的可怕之处在于：编译器和运行时不再需要对你的程序做任何保证，某次优化、某个平台、某个负载下才爆，是常见现象。 2.2 原子性（Atomicity）与临界区（Critical Section）并发错误经常出现在一种场景里：一段逻辑看起来像“一个操作”，实际上是多个步骤。例如 counter++，你在语义上想表达“加一”，但在机器层面通常会拆成三步：读（load）→ 改（add）→ 写（store）。 这类“必须整体不可分割”的代码片段，就是我们在并发里反复提到的 临界区（critical section）：它访问共享变量或共享资源，不能被多个线程同时执行。OSTEP 给出的定义非常直接：临界区是一段访问共享资源的代码，必须确保同一时间最多一个线程进入。 进一步地，我们真正想要的是让临界区“看起来像一条原子指令一样执行”（也就是把一串指令当作不可被并发打断的整体）。OSTEP 在讲锁的基本思想时也强调：加锁的目标就是让临界区“仿佛作为一条单独的原子指令执行”。 （这里你可以放一张时序图：把 counter++ 拆成 loadaddstore，两条线程交错后导致丢失更新。） 2.3 示例：多线程下的自增操作为什么线程不安全现在用最经典的例子把“交错执行”带来的问题落地：多个线程同时对一个共享计数器做 ++。 #include iostream#include threadint a = 0; // 共享变量void add_one_million() for (int i = 0; i 1000000; ++i) ++a; // data race: 未同步的并发写 int main() std::thread t1(add_one_million); std::thread t2(add_one_million); t1.join(); t2.join(); std::cout expected: 2000000 ; std::cout actual: a ; return 0; 直觉上，如果 2 个线程各自执行 100 万次 ++，结果应该是 200 万。但你把这段代码放到多线程里跑，常常会发现：最终结果小于预期，而且每次运行还可能不一样。 关键原因在于：**++**** 看起来是“一步”，其实通常可以拆成三步完成**： 读出来：把变量当前的值从内存读到 CPU 的寄存器里 加一：在寄存器里把这个值 +1 写回去：把加完后的结果再写回内存中的变量 单线程时，这三步总是按顺序完成，没问题；但在多线程时，两个线程可能会在这三步中交错执行。一旦发生“交错”，就可能出现这种情况： 线程 1 先读到 a = 100（还没来得及写回） 线程 2 也读到 a = 100 线程 1 加一写回 a = 101 线程 2 也加一写回 a = 101 你看，两个线程都做了“加一”，但结果只从 100 变成了 101，本该变成 102 的那一次增量就丢了。这就是为什么多线程下 ++ 会出现“算少了”的现象。 更关键的是：在 C++ 的语义里，这种写法不仅“可能算错”，而是直接构成 data race，行为未定义。cppreference 的多线程与数据竞争章节甚至把这个例子作为典型反例，并明确标注为 undefined behavior；同时也展示了把 int 换成 std::atomicint 后就变为良定义行为。 :::info这不是概率问题。并发下指令交错是常态，你只能控制交错发生时是否仍然正确，而不能指望它别交错。这种现象被称为 indeterminate：输出随线程运行时序变化而变化。 ::: 2.4 从“线程安全”自然过渡到“可重入”（避免引入突兀）到这里，你基本已经能形成一个清晰的判断标准：所谓线程安全，核心就是“并发调用时仍然正确”，通常意味着你要么避免共享可变状态，要么用同步手段建立正确的执行关系。 而**可重入（re-entrant）**经常被拿来和线程安全一起出现，但它关注的是另一个维度：某个操作在“尚未完成时”又被再次进入，是否仍然正确。 比如：同一个函数正在执行过程中（可能因为中断、信号处理、回调、递归等原因）再次进入这个函数，如果它内部依赖某些隐藏的可变共享状态（静态缓冲区、全局临时变量等），就可能出问题。 二者的区分很清晰：线程安全是“多线程同时调用也安全”；可重入是“在一次调用尚未完成时再次调用也安全”，因此可重入通常是更强的要求。另外，POSIX 的白皮书也给出了“reentrant function”的规范性表述（强调多线程交错调用时效果应等价于某种串行顺序）。 2.5 活性问题：死锁活锁饥饿并发还有一类更隐蔽的风险：程序不再前进（lack of progress）。典型包括： Deadlock（死锁）：所有线程都卡住了，互相等待资源，谁也无法继续。 CS537 的讲义用一句话概括：一种策略让所有线程都“stuck”，无人能前进。 另一个讲义进一步强调了死锁的本质：每个实体都在等待别人持有的资源，形成等待环。 死锁的发生必须同时满足四个必要条件：互斥条件、请求与保持条件、不剥夺条件和循环等待条件。 Starvation（饥饿）：不是所有线程都卡住，而是某个线程可能无限期等不到运行或资源。 CS537 讲义把它描述为：策略导致某些线程在某些情况下长期不执行。 Livelock（活锁）：所有线程都在“忙”，但一直在做无效动作，仍然没有进展。 CS537 讲义的表述非常直观：大家一直做事，但永远无法取得进展。 线程互斥（Mutual Exclusion）原子操作（Atomic）：什么时候“无需锁”，什么时候“必须锁”在上一章你已经看到：对普通 int 做并发 ++，不仅“可能算错”，在 C++ 语义里更是 data race → 未定义行为。这一章我们把视角切到 std::atomic：它能把哪些问题变成“良定义”，又在哪些地方绝对不该用 atomic 代替锁。 std::atomic 的基本语义（C++17）std::atomicT 的核心承诺很简单：对同一个原子对象的并发访问，语言层面给出清晰语义。你最常用的是两类操作： 原子读写：load() store() 原子读-改-写（RMW, read-modify-write）：把“读旧值 + 计算新值 + 写回”合成一个不可分割的整体，例如 fetch_add（以及常见的 ++counter）。在文档里，fetch_add 被明确标注为 read-modify-write 操作。 这就是为什么把共享变量从 int 换成 std::atomicint 后，多线程下的累加能稳定得到正确结果：你不再是在并发环境里“拆成三步地更新”，而是在做语言定义的原子 RMW。 #include iostream#include thread#include atomicstd::atomicint a0;void add_one_million() for (int i = 0; i 1000000; ++i) ++a; // data race: 未同步的并发写 int main() std::thread t1(add_one_million); std::thread t2(add_one_million); t1.join(); t2.join(); std::cout expected: 2000000 ; std::cout actual: a ; return 0; :::infoatomic 解决的是“单变量原子性”，不是“任意逻辑一致性”把一个变量做成 atomic，只能保证这个变量的每次读写更新是良定义的；如果你的正确性依赖“多个共享状态一起保持某种关系”，那就已经超出 atomic 的能力边界。 ::: 在某些设置下（比如开启优化、跑在多核 CPU 上），编译器和 CPU 可能会为了性能，把原子操作及其周围的普通读写，在“对外可见的顺序”上做调整。你写代码时看到的是“先 A 再 B”，但另一个线程观察到的效果（例如被编译器优化了后），可能更像“B 先发生、A 后发生”。这类现象通常就用一个词概括：指令重排序。 它会造成非常经典、也最容易理解的并发 bug：“信号先到，数据后到”。例如你在线程 A 里写： 先把数据写好（普通变量） 再把 ready = true（原子变量）当作“通知” 直觉上，线程 B 只要看到 ready == true，就应该能读到最新数据；但如果没有足够的约束，B 可能出现：看到了 readytrue，却还没看到数据更新的情况。这里不是说代码“乱执行”，而是说跨线程观察时，写入被看见的先后顺序可能不符合你的直觉。 为了解决这种“顺序可见性”问题，C++ 给原子操作设计了一个参数：std::memory_order，用来规定原子操作周围的内存访问应该按多强的规则排序。 而你不显式写 memory_order 时，标准库的默认行为是：按顺序一致（**memory_order_seq_cst**）来处理。你可以把它理解为“最保守、最不容易写错、也最容易推理”的默认规则：让不同线程看到的原子操作顺序尽量一致，从而降低“信号先到、数据后到”这种反直觉现象。 （像 fetch_add 这类原子 RMW 操作也支持指定 order，默认不写就走上述默认规则。） CAS（Compare-And-SwapExchange）前面我们用 fetch_add 修复了多线程自增问题，这很好理解：“加一”这种规则太常见了，所以标准库直接给了现成接口。但我们很快会遇到下一类需求： 我不想“加一”，我想按自己的规则更新：例如“把值更新成更大的那个”“只有满足某个条件才更新”“基于旧值算出新值再写回”。标准库没给现成接口怎么办？ CAS 在干什么？先从“并发下值会变”说起在并发环境里，你常常会遇到一个事实：你刚读到的值，下一秒可能就被别的线程改了。 所以问题变成：我能不能这样做—— 我先读到一个值 expected 我基于它计算出一个新值 desired 但在写回之前，我希望确认：“它还是不是我刚才看到的那个值？” 如果不是，那说明有人插队修改过，我就不能盲目写回。 CAS 就是用来解决这一点的。它的意思可以用一句话说清楚：**“如果当前值还等于 **expected**，就把它改成 **desired**；否则不改，并把实际值告诉我。”**在 C++ 里，这个操作叫 compare_exchange_weak/compare_exchange_strong（compare-and-exchange）。 为什么 CAS 总是和“循环”绑定？因为 CAS 的失败并不是异常情况，反而是并发下的常态： 你读到 expected 别的线程抢先把值改了 你 CAS 失败 失败时 **expected** 会被更新成“当前真实值”，你再基于新值计算、继续尝试 所以 CAS 的标准用法几乎都是：读-算-尝试（失败就重试）。 :::infocompare_exchange_weak 允许一种现象：即使当前值等于 expected，它也可能返回失败（spurious failure）。所以 weak 版本通常就是写在循环里重试，而且在一些平台上它可能更高效。我们可以把它当成一条写法规则：weak 默认就要放在循环里；strong 用在你真的希望“相等就尽量成功”的地方。 ::: 示例：用 CAS 实现 “原子更新最大值”多个线程同时更新一个共享变量 a，希望它最终等于出现过的最大值（a = max(a, v)）。 #include atomic#include thread#include vector#include iostreamvoid atomic_update_max(std::atomicint a, int v) int expected = a.load(); // 先读一个旧值 while (expected v // 只有 v 更大才需要更新 !a.compare_exchange_weak( expected, v)) // 失败时 expected 会被写成“当前真实值” // 什么也不用做，继续循环即可： // 1) expected 已被更新为最新值 // 2) 重新判断 expected v // 3) 再尝试 CAS int main() std::atomicint a0; std::vectorstd::thread ts; for (int i = 0; i 8; ++i) ts.emplace_back([a, i] // 每个线程尝试把 a 更新成一个不同的值 atomic_update_max(a, i * 10 + 7); ); for (auto t : ts) t.join(); std::cout final a = a.load() ; // 期望是最大值：77 这段代码最重要的两点是： CAS 保证“检查 + 更新”是一个整体：只有当 a 仍等于 expected 时才写入新值，否则就失败。 失败不是坏事：失败时 expected 会自动变成最新值，让你能基于最新状态再算一次、再试一次。 atomic 的边界：为什么“复合不变量”仍需要锁std::atomic 很容易被用过头：把 x、y、state、size 统统改成 atomic，然后下意识觉得“那整体就线程安全了”。问题在于，atomic 提供的是单个变量的原子性，而此时我们真正想要的往往是多个变量之间的整体一致性。 先从“并发下最容易发生的事”说起：你的一次更新如果要改两个变量，哪怕每次 store() 都是原子的，这两次写入之间仍然存在一个窗口。在这个窗口里，另一个线程完全可能插进来读数据，于是它就会读到一个“半更新”的中间态。 举个最简单的例子：你希望永远满足 x y，写线程要做的是“同时调整 x 和 y”。但现实是它只能先改一个，再改另一个；读线程只要恰好夹在中间，就能看到临时违反不变量的组合，比如 x 已经变大，而 y 还没来得及跟上。这里并不是 atomic “失效”，而是：你需要的是一次事务式更新（要么都变，要么都不变），atomic 给不了这种跨变量的一致性快照。 更进一步，在多核系统里，如果你对多个变量进行读写而没有足够的同步约束，不同线程观察到“值变化的顺序”甚至可能不一致：一个线程看到 x 先变，另一个线程看到 y 先变。cppreference 对 memory_order 的说明就明确提到：当多个线程同时读写多个变量时，一个线程可能观察到的变化顺序不同于另一个线程写入顺序，甚至不同读线程之间观察到的顺序也可能不同。 这句话的直观含义是：当你在做“复合状态更新”时，如果没有把它变成一个整体的同步单元，读者看到的世界可能是“拼接出来的”。 所以一旦我们的正确性条件是跨多个共享状态的，例如： x y 必须始终成立 一个对象的多个字段必须同时更新才算“进入新状态” map 与旁路索引必须一致 那么单靠 atomic 往往会把你带到“局部都很原子，但整体仍可能乱”的局面：每个字段单独读写都没 data race，可读到的组合却可能不一致。 :::infoatomic 也不保证一定“无锁”。cppreference 明确写到：除了 std::atomic_flag 外，其它 atomic 类型允许用 mutex 或其它锁机制实现；是否 lock-free 需要看 is_lock_free()is_always_lock_free 的结果。 ::: 3.3 性能陷阱：伪共享（False Sharing）:::info读者提示：理解下面的“伪共享（false sharing）”需要你先知道三件事：1）CPU 缓存以 cache line 为最小管理单位；2）多核通过 缓存一致性协议维护 cache line 的共享与失效；3）多数 CPU 采用 write-back写缓冲，写入需要通过一致性协议传播到其他核心。伪共享的本质就是：两个线程改的是不同变量，但它们落在同一条 cache line 上，导致一致性协议把整条 cache line 当成“共享热点”来回同步，从而白白产生开销。 ::: In computer science, false sharing is a performance-degrading usage pattern that can arise in systems with distributed, coherent caches at the size of the smallest resource block managed by the caching mechanism. When a system participant attempts to periodically access data that is not being altered by another party, but that data shares a cache block with data that is being altered, the caching protocol may force the first participant to reload the whole cache block despite a lack of logical necessity. The caching system is unaware of activity within this block and forces the first participant to bear the caching system overhead required by true shared access of a resource. By far the most common usage of this term is in modern multiprocessor CPU caches, where memory is cached in lines of some small power of two word size (e.g., 64 aligned, contiguous bytes). If two processors operate on independent data in the same memory address region storable in a single line, the cache coherency mechanisms in the system may force the whole line across the bus or interconnect with every data write, forcing memory stalls in addition to wasting system bandwidth. In some cases, the elimination of false sharing can result in order-of-magnitude performance improvements.False sharing is an inherent artifact of automatically synchronized cache protocols and can also exist in environments such as distributed file systems or databases, but current prevalence is limited to RAM caches. 另一个常见误解是：把锁换成 atomic，就一定更快。并发下经常出现相反情况：atomic 写得越频繁，越可能越慢。 原因往往不在“原子指令本身”，而在缓存一致性：当多个线程频繁写同一个 cache line 上的数据时，该 cache line 会在不同 CPU 核之间反复失效同步，吞吐下降、延迟抖动。这种“两个线程写的是不同变量，但它们恰好落在同一条 cache line 上，于是互相拖累”的现象，就叫 伪共享（false sharing）。 std::mutex 与为什么不要手写 lock()/unlock() 在上一 节我们已经见过“共享变量并发写会出事”。接下来我们需要一个更工程化的工具：把“临界区”围起来，让同一时刻只有一个线程能进入。C++ 标准库提供的核心原语是 std::mutex，以及一组围绕 RAII 设计的锁管理器（lock_guard/unique_lock/scoped_lock/...）。 std::mutex 的职责很单纯：保护共享数据，避免被多个线程同时访问。问题在于：如果我们手写 lock() unlock()，一旦代码中途 return、throw、或未来某次维护时忘了写 unlock()，就会把互斥量“永远锁住”，其他线程会一直等下去。 下面这段对比代码展示了这个风险：反例里我们故意 throw，然后用 try_lock() 观察互斥量是否还被锁着（避免程序真的卡死）；正例用 RAII 的 std::lock_guard，即使抛异常也会在析构时自动释放。lock_guard 的语义是“构造时加锁、离开作用域析构时解锁”。 #include iostream#include mutex#include stdexceptstd::mutex g_m;void bad() g_m.lock(); throw std::runtime_error(oops); // 永远到不了 unlock g_m.unlock();void good() std::lock_guardstd::mutex lk(g_m); // RAII：作用域结束必解锁 throw std::runtime_error(oops);int main() // 反例：手写 lock/unlock，异常后锁被“遗留” try bad(); catch (...) if (g_m.try_lock()) std::cout [bad] mutex is NOT locked (unexpected) ; g_m.unlock(); else std::cout [bad] mutex is still locked - other threads would block ; // 为了不影响后续演示，这里把它解开 g_m.unlock(); // 正例：lock_guard，异常后锁仍会被释放 try good(); catch (...) if (g_m.try_lock()) std::cout [good] mutex is unlocked (expected) ; g_m.unlock(); else std::cout [good] mutex is still locked (unexpected) ; :::info我们可以把锁理解为“锁住共享数据的访问权”，而不是“锁住某段代码”。工程上更推荐把 mutex 和它保护的数据一起封装进类，减少“忘记加锁锁错对象”的概率。 ::: std::lock_guard：默认首选std::lock_guard 是最轻量的 RAII 锁管理器：构造时尝试获取互斥量所有权，离开作用域就释放。它不可拷贝，也不提供中途解锁等复杂能力，所以简单、稳定、开销低。 #include iostream // std::cout#include thread // std::thread#include mutex // std::mutex, std::lock_guard#include stdexcept // std::logic_errorstd::mutex mtx;void print_even (int x) if (x%2==0) std::cout x is even ; else throw (std::logic_error(not even));void print_thread_id (int id) try // using a local lock_guard to lock mtx guarantees unlocking on destruction / exception: std::lock_guardstd::mutex lck (mtx); print_even(id); catch (std::logic_error) std::cout [exception caught] ; int main () std::thread threads[10]; // spawn 10 threads: for (int i=0; i10; ++i) threads[i] = std::thread(print_thread_id,i+1); for (auto th : threads) th.join(); return 0; std::unique_lock：为条件变量与更复杂流程准备std::unique_lock 的定位是“更通用的锁所有权管理器”：它支持延迟加锁、try_lock、中途 unlock 重新 lock，还可以移动（transfer ownership）。 下面这段代码展示两个常用模式：延迟加锁（defer_lock）和缩短持锁时间（中途 unlock 把重活放到锁外做）。 #include mutexstd::mutex m;void do_work() std::unique_lockstd::mutex lk(m, std::defer_lock); // 先不锁 // ...做一些不需要锁的准备工作 lk.lock(); // 真正进入临界区 // 修改共享数据（需要锁保护） lk.unlock(); // 尽早放锁 // 重活放在锁外：IO、计算、回调等 多把锁的标准解：std::scoped_lock std::lock多把锁最常见的风险是死锁：线程 A 先锁 m1 再锁 m2，线程 B 反过来先锁 m2 再锁 m1，两边互相等待就卡住。C++17 推荐的写法是：要么统一顺序，要么一次性锁多把。 std::scoped_lock 是 C++17 引入的 RAII 包装器，支持“持有 0 或多把互斥量”。std::lock 是更底层的“一次性锁多把”的函数，cppreference 也明确提示：scoped_lock 是它的 RAII 包装器，一般更推荐。 #include mutexstruct Account std::mutex m; int balance0;;void transfer(Account from, Account to, int amount) std::scoped_lock lk(from.m, to.m); // C++17：一次性锁多把 from.balance -= amount; to.balance += amount; #include mutexvoid transfer2(Account from, Account to, int amount) std::unique_lockstd::mutex l1(from.m, std::defer_lock); std::unique_lockstd::mutex l2(to.m, std::defer_lock); std::lock(l1, l2); // 一次性获取两把锁 from.balance -= amount; to.balance += amount; :::info多锁场景我们只保留两条纪律：统一顺序，或使用 scoped_lock/std::lock。不要随手写“先锁 A 再锁 B”的不一致逻辑。 ::: 读多写少：std::shared_mutex + std::shared_lock当共享数据“读远多于写”时，普通 mutex 会把读也串行化，浪费并发性。C++17 提供 std::shared_mutex：它支持两种模式： 共享（shared）：多个线程可同时持有（读锁） 独占（exclusive）：同一时刻只能一个线程持有（写锁） std::shared_lock 用于管理共享锁；而独占锁仍然用 std::unique_lockshared_mutex。 #include shared_mutex#include unordered_mapclass ThreadSafeMap public:int get(int k) const std::shared_lockstd::shared_mutex lk(mu_); // 读：共享锁 auto it = m_.find(k); return it == m_.end() ? 0 : it-second;void set(int k, int v) std::unique_lockstd::shared_mutex lk(mu_); // 写：独占锁 m_[k] = v;private:mutable std::shared_mutex mu_;std::unordered_mapint, int m_;; 配图建议（读多写少示意图）画三个读者同时持有 shared_lock（绿色），一个写者请求独占锁（红色），并标注“写者需等待读者全部释放”。shared_mutex::lock() 的说明里也强调：当已有共享锁独占锁存在时，独占加锁会阻塞。 :::info公平性与“写者是否可能饥饿”依实现与策略而定，不要默认“写一定很快拿到锁”。 ::: 线程同步（Synchronization）条件变量：从忙等到阻塞等待为什么需要 std::condition_variable如果我们在队列空的时候用 while (empty) {} 这种方式反复检查，就属于忙等：CPU 会空转而被白白消耗，而且线程越多锁越热。条件变量的目的就是把这种“反复检查”变成：条件不成立就睡眠阻塞；条件成立被唤醒后再检查一次。 std::condition_variable 的定位是：配合 std::mutex，让线程在某个条件满足之前阻塞等待，并由其他线程在修改条件后发出通知。 wait(lock, predicate) 的标准写法与虚假唤醒条件变量的 **wait** 返回，只能说明“现在应该再检查一次条件”，而不能说明“条件已经满足”。原因是：标准库允许线程在没有收到 notify_one()/notify_all() 的情况下，从 wait 中返回——这就叫虚假唤醒（spurious wakeup）。cppreference 在描述 std::condition_variable 的等待行为时也明确提到：wait 会挂起线程，直到被通知、超时，或发生虚假唤醒，然后再重新获取互斥锁返回。 为什么要允许这种行为？我们不需要钻到内核细节里，只要记住它是“实现层面为了兼容性与效率而允许的行为”。因此它不是程序逻辑错误，也不是异常能捕获的情况；我们要做的是写出天然兼容虚假唤醒的等待代码。 规范写法：醒来以后必须重新检查条件最常见的坑是把等待写成 “if + wait”： std::unique_lockstd::mutex lk(m);if (q.empty()) // 只检查一次 cv.wait(lk); // 可能因为虚假唤醒返回// 如果这里仍然是空队列，下面就出错了auto x = q.front();q.pop(); 这段代码的问题在于：我们只在进入 wait 前检查了一次 q.empty()。如果线程“醒来”时队列仍为空（可能是虚假唤醒，也可能是别的线程先把数据拿走了），我们仍会继续执行 front()/pop()，逻辑就崩了。 正确写法有两种，本质完全一样： 1）推荐写法：用带谓词的 **wait** 重载它把“醒来后再次检查条件”封装好了： std::unique_lockstd::mutex lk(m);cv.wait(lk, [] return !q.empty(); ); // 条件不满足就继续等auto x = q.front();q.pop(); 这个重载就是用来忽略处理虚假唤醒的。 2）等价写法：手写 while 循环更直白： std::unique_lockstd::mutex lk(m);while (q.empty()) cv.wait(lk);auto x = q.front();q.pop(); 这两种写法的核心思想是一样的：把 **wait** 当成“可能会醒”的睡眠，而把 predicatewhile 当成“真正的条件门槛”。醒来只是提示我们“再看一眼共享状态”，而不是承诺“状态已经满足”。 :::info 等待条件变量时，**默认使用 ****cv.wait(lock, predicate)**（最不容易写错）。 如果必须用不带 predicate 的 wait，也必须写成 while 重新检查条件，**禁止 ****if + wait**。 这样写不仅能规避虚假唤醒，也能顺带规避另一类常见问题：通知与条件检查之间的竞态（醒来时条件未必仍成立），整体更稳。 ::: 信号量（Semaphore）信号量是什么：计数许可如果说 mutex 是“同一时刻只能一个人进”，信号量更像“发放若干张许可证”：许可证 0 时允许进入并扣减一张；许可证为 0 时就阻塞等待。这个抽象通常称为计数信号量（counting semaphore）。值只在 01 的情形称为二值信号量（binary semaphore）。 semaphore vs mutexcv：各自擅长什么我们可以用一句话区分三者的“表达力”： mutex：保护临界区一致性（互斥进入） condition_variable：等待“某个条件成立”（需要 mutex + predicate） semaphore：表达“资源数量并发许可”（天然就是计数） Tip信号量通常不直接保证数据结构一致性：它解决“名额数量”，而共享结构的读写仍可能需要 mutex 来保护。 规范写法标准库的 std::counting_semaphore semaphore 是 C++20 才引入的。在 C++17 里，我们通常做两件事： 正文讲概念与用法 需要代码时，用 mutex + condition_variable + 计数器 模拟一个最小可用的 semaphore（写法与 wait(lock,pred) 一样，自动处理虚假唤醒） 下面给出一个最小实现，以及一个“限制最多 K 个线程同时访问资源”的 demo。 #include condition_variable#include mutexclass Semaphore public:explicit Semaphore(int initial) : count_(initial) void acquire() std::unique_lockstd::mutex lk(m_); cv_.wait(lk, [] return count_ 0; ); --count_;void release() std::lock_guardstd::mutex lk(m_); ++count_; cv_.notify_one();private:std::mutex m_;std::condition_variable cv_;int count_;; #include atomic#include chrono#include iostream#include thread#include vector// 假设 Semaphore 已定义int main() Semaphore sem(3); // 最多 3 个线程同时进入 std::atomicint in_flight0; std::vectorstd::thread ts; for (int i = 0; i 10; ++i) ts.emplace_back([, i] sem.acquire(); int cur = ++in_flight; std::cout enter i , in_flight= cur ; std::this_thread::sleep_for(std::chrono::milliseconds(200)); cur = --in_flight; std::cout leave i , in_flight= cur ; sem.release(); ); for (auto t : ts) t.join(); 如果运行正常，我们会看到 in_flight 的最大值不超过 3。 三个经典同步问题 本章记录“生产者-消费者 读者-写者 哲学家就餐”三个经典问题，目的是用它们把互斥与同步工具串起来，帮助我们建立可复用的并发思维框架。需要提前说明的是：这些问题在不同约束下都有多种解法（例如偏吞吐、偏公平、偏实现简单、偏可验证），本文给出的实现更强调“语义清晰、可读性强、易于推理”，并不保证在所有平台与负载下都是性能最优或策略最公平的版本。读者可以把它们当作参考基线，在理解原理后再按具体业务目标做取舍与优化。 前面我们已经把两类“工具”准备好了： **线程互斥：**用锁把临界区围起来，保证共享数据的一致性 **线程同步：**用条件变量信号量让线程在“条件不满足时阻塞等待”，而不是忙等 这一章我们把它们落到三个经典模型上。重点是抓住“共享状态是什么、什么时候等、什么时候唤醒”。 生产者-消费者（有界阻塞队列）这个问题对应非常常见的工程场景：一个线程（或多个）产生任务数据，另一个线程（或多个）消费任务数据，中间用一个缓冲区（队列）连接。核心矛盾只有两个： 队列空：消费者不能硬取，需要等待 队列满：生产者不能硬塞，需要等待 这里最容易写错的点，是把等待写成“if + wait”。我们已经强调过：wait 可能因为通知、也可能因为虚假唤醒返回，所以正确写法必须醒来后重新检查条”。 我们先把“队列的共享状态”说清楚：队列的大小 q.size() 决定了两个条件： not_empty：!q.empty()（消费者可以取） not_full：q.size() cap（生产者可以放） 下面是一个 C++17 的解法：一个有界 BlockingQueueT，用一把 mutex 保护队列结构，用两个条件变量分别表达 not_empty 与 not_full（一个常见的用法，值得记住）。 #include condition_variable#include mutex#include queue#include utilitytemplate class Tclass BlockingQueue public:explicit BlockingQueue(size_t cap) : cap_(cap) void push(T v) std::unique_lockstd::mutex lk(m_); cv_not_full_.wait(lk, [] return q_.size() cap_; ); // 等 not_full q_.push(std::move(v)); lk.unlock(); cv_not_empty_.notify_one(); // 状态从 empty - non-empty 的方向推进T pop() std::unique_lockstd::mutex lk(m_); cv_not_empty_.wait(lk, [] return !q_.empty(); ); // 等 not_empty T v = std::move(q_.front()); q_.pop(); lk.unlock(); cv_not_full_.notify_one(); // 状态从 full - non-full 的方向推进 return v;private:const size_t cap_;std::mutex m_;std::condition_variable cv_not_full_;std::condition_variable cv_not_empty_;std::queueT q_;; 读者-写者（读并发、写独占）我们要解决的场景很典型：同一份共享数据上，读操作彼此并不冲突，所以理想情况下可以并发执行；但写操作会改变共享数据，它必须独占，并且与任何读互斥。换句话说： 允许多个读者同时进入 一旦有写者进入，必须保证没有任何读者写者同时在里面 如果我们直接用普通 std::mutex，当然能做到正确性，但代价是：读也被串行化——明明读之间互不影响，却被迫排队，浪费并发性。C++17 的 std::shared_mutex 就是为此准备的：读者拿共享锁（std::shared_lock），写者拿独占锁（std::unique_lockstd::shared_mutex）。不过，标准库的 shared_mutex 并不承诺“读者优先写者优先严格公平”这类策略；如果我们确实需要可控策略，就要自己实现一把 RWLock（读写锁）。 下面是三种常见策略： 读者优先 RWLock（Readers-preference） 只要没有写者正在写，就尽量放行读者；写者必须等到“所有读者都离开”。优点是读吞吐高，缺点是读源源不断时写者可能等很久。 写者优先 RWLock（Writers-preference） 一旦有写者在排队，就不再放行新的读者；这样写者不会被“不断涌入的读”饿死。优点是写延迟可控，缺点是写多时读延迟会升高。 公平 RWLock（Phase-fair，读写轮转） 让读和写“轮流占用舞台”。当读者批量完成后让一位写者进入，写者完成后再放行一批读者。它不追求严格 FIFO，但能避免某一方长期饥饿，同时实现复杂度可控。 哲学家就餐（多锁死锁与规避） 哲学家就餐问题可以这样表述，假设有五位哲学家围坐在一张圆形餐桌旁，做以下两件事情之一：吃饭，或者思考。吃东西的时候，他们就停止思考，思考的时候也停止吃东西。餐桌上有五碗意大利面，每位哲学家之间各有一只餐叉。因为用一只餐叉很难吃到意大利面，所以假设哲学家必须用两只餐叉吃东西。他们只能使用自己左右手边的那两只餐叉。哲学家就餐问题有时也用米饭和五根筷子而不是意大利面和餐叉来描述，因为吃米饭必须用两根筷子。 这个问题不考虑意大利面有多少，也不考虑哲学家的胃有多大。假设两者都是无限大。 问题在于如何设计一套规则，使得在哲学家们在完全不交谈，也就是无法知道其他人可能在什么时候要吃饭或者思考的情况下，可以在这两种状态下永远交替下去。 哲学家就餐用来演示一个非常典型的活性风险：死锁。场景是环形资源：每个人需要两把叉才能吃。如果所有人都按同一种顺序拿叉（比如都先拿左叉），就可能出现： 每个人都拿到一把叉 然后都在等待另一把叉 形成环路等待，系统没有任何线程能推进 这一题的价值在于：解决“多把锁怎么拿才不会死锁”的问题。我们的原则很简单： 多锁要么统一顺序 要么一次性拿（交给标准库的死锁规避算法） 可以使用 std::scoped_lock 进行一种标准的实现： #include algorithm#include mutex#include thread#include vectorint main() constexpr int N = 5; std::mutex forks[N]; auto philosopher = [](int i) int left = i; int right = (i + 1) % N; int a = std::min(left, right); int b = std::max(left, right); std::scoped_lock lk(forks[a], forks[b]); // C++17：一次性锁两把（带死锁规避） // eat... ; std::vectorstd::thread ts; for (int i = 0; i N; ++i) ts.emplace_back(philosopher, i); for (auto t : ts) t.join(); 我们把“统一顺序”说得更直白一点：所有线程都遵守“先拿编号小的，再拿编号大的”，就不可能形成环路等待。再加上 scoped_lock 的“一次性上锁”，实现上更稳。 参考资料melonstreet - 博客园 进程控制块PCB结构体 task_struct 描述 - ProLyn - 博客园 Process Control Block in OS - GeeksforGeeks Linux内存管理4—虚拟地址空间管理 - jasonactions - 博客园 Starvation and Livelock - GeeksforGeeks https://en.wikipedia.org/wiki/False_sharing https://en.cppreference.com/w/ https://cplusplus.com/reference/mutex/lock_guard 哲学家进餐问题(Dining philosophers problem) https://zh.wikipedia.org/wiki/%E5%93%B2%E5%AD%A6%E5%AE%B6%E5%B0%B1%E9%A4%90%E9%97%AE%E9%A2%98","tags":["cpp","os"]},{"title":"Hello World","path":"/blog/2026/02/01/hello-world/","content":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post$ hexo new My New Post More info: Writing Run server$ hexo server More info: Server Generate static files$ hexo generate More info: Generating Deploy to remote sites$ hexo deploy More info: Deployment","tags":["Hexo"],"categories":["入门"]},{"title":"关于","path":"/blog/about/index.html","content":"关于你好，我是 Kamil Liu，软件工程背景的开发者，关注网络安全与后端系统。Hi, I’m Kamil Liu, a software-engineering-trained developer focused on cybersecurity and backend systems. 本科就读天津大学软件工程（预计 2026 年 6 月毕业），将于 2026 年 8 月入读南洋理工大学网络安全硕士。I study Software Engineering at Tianjin University (expected Jun 2026) and will start an M.Sc. in Cyber Security at NTU in Aug 2026. 我喜欢把复杂需求拆成稳定的数据链路与可维护的服务结构。I like turning complex needs into reliable data pipelines and maintainable service structures. 做过海外交易所接入与微服务适配，重点处理限流、数据异构与一致性。I built exchange integrations for microservices, focusing on rate limits, heterogeneous data, and consistency. 参与无人机通信仿真项目，做跨语言集成与服务治理，保证链路指标稳定。I worked on UAV communication simulation, handling cross-language integration and service governance. 毕业设计研究多专家协同的时间序列预测，关注趋势、周期与异常。My thesis explored multi-expert time-series forecasting for trends, seasonality, and anomalies. 常用技术：Java C#，SpringBoot、.NET、gRPC、Consul、Redis、MySQL。Core stack: JavaC#, SpringBoot, .NET, gRPC, Consul, Redis, MySQL. 做事风格轻量务实，追求清晰、可复用、少踩坑。My style is lightweight and practical—clear, reusable, fewer detours. 这里记录安全、后端与系统工程的学习与实践。This blog tracks my learning and practice in security, backend, and systems engineering. 邮箱：kamill7779@outlook.comEmail: kamill7779@outlook.com"},{"title":"友链","path":"/blog/friends/index.html","content":"欢迎交换友链，可以通过邮件或 GitHub 联系我。 朋友们HexoStellarMDN Web Docs 站点推荐Hexo 官方Hexo 官方网站与文档Stellar 文档Stellar 主题使用说明与示例GitHub代码托管与协作平台"},{"title":"时间线","path":"/blog/timeline/index.html","content":"2026-02搭建博客与主题配置，完成首页结构优化。发布作品集与时间线页面。2025-12完成第一个可复用组件库原型。将学习笔记整理为专题。2025-06开始系统化整理算法与工程实践。"}]